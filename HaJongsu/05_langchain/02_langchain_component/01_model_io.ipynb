{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMgkwxlJKdQgM0e/8FXN2do"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Model IO\n","- Language Model\n","- Prompt\n","- OutputParser\n","![](https://d.pr/i/Wy5B5B+)"],"metadata":{"id":"wWwkYnuxfpML"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EQE9aRP4fins","executionInfo":{"status":"ok","timestamp":1751001851196,"user_tz":-540,"elapsed":9931,"user":{"displayName":"하종수","userId":"15198303925242776010"}},"outputId":"eef31ea7-8cf6-4e70-f0d8-dc42b0d2bba1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n","Requirement already satisfied: langchain-openai in /usr/local/lib/python3.11/dist-packages (0.3.26)\n","Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.26)\n","Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.11/dist-packages (0.3.0)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.66)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n","Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.1)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.91.0)\n","Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n","Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n","Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n","Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n","Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.1)\n","Requirement already satisfied: huggingface-hub>=0.30.2 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.33.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.5.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (2025.3.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (24.2)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (4.67.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (4.14.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (1.1.5)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n","Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.6.15)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n"]}],"source":["!pip install langchain langchain-openai langchain-community langchain-huggingface"]},{"cell_type":"code","source":["# colab secret 모두 등록할 것\n","from google.colab import userdata\n","import os\n","\n","os.environ['LANGSMITH_TRACING'] = userdata.get('LANGSMITH_TRACING')\n","os.environ['LANGSMITH_ENDPOINT'] = userdata.get('LANGSMITH_ENDPOINT')\n","os.environ['LANGSMITH_API_KEY'] = userdata.get('LANGSMITH_API_KEY')\n","os.environ['LANGSMITH_PROJECT'] = userdata.get('LANGSMITH_PROJECT')\n","os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"],"metadata":{"id":"wsvsvcBHgAXN","executionInfo":{"status":"ok","timestamp":1751001853782,"user_tz":-540,"elapsed":2578,"user":{"displayName":"하종수","userId":"15198303925242776010"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## Language Models\n","\n","https://python.langchain.com/api_reference/reference.html#integrations\n","\n","LangChain의 Integrations 섹션에서는 다양한 다운스트림 LLM 모델과의 연동을 지원하다.\n","\n","이 섹션에서는 OpenAI, Hugging Face, GPT-4 등의 다양한 LLM 모델과 LangChain을 연결하는 방법을 다룬다."],"metadata":{"id":"CF4MGf5agWwp"}},{"cell_type":"markdown","source":["### openai"],"metadata":{"id":"wBo-enq7xcxw"}},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","\n","llm = ChatOpenAI(model='gpt-4o')\n","\n","llm.invoke('태국의 수도는 어디인가요?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yEJJTiW8gNlx","executionInfo":{"status":"ok","timestamp":1751001856513,"user_tz":-540,"elapsed":2727,"user":{"displayName":"하종수","userId":"15198303925242776010"}},"outputId":"678a8fae-908f-4ea8-8a1d-de2c1e014490"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='태국의 수도는 방콕입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 16, 'total_tokens': 26, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-BmvrLk8RbPS7Gn7yy8n1SZqiEw1CL', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--4305ce74-c4c4-4b52-9030-006d90fcaceb-0', usage_metadata={'input_tokens': 16, 'output_tokens': 10, 'total_tokens': 26, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["### huggingface"],"metadata":{"id":"_3MY5d_8yVT9"}},{"cell_type":"code","source":["from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n","\n","llm = HuggingFaceEndpoint(\n","    repo_id = 'microsoft/Phi-3-mini-4k-instruct',\n","    task = 'text-generation'\n",")\n","\n","chat_model = ChatHuggingFace(llm = llm, verbose=True)\n","\n","chat_model.invoke('프랑스의 수도는 어디인가요?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"UrLljrFTx4pk","executionInfo":{"status":"error","timestamp":1751001858417,"user_tz":-540,"elapsed":1902,"user":{"displayName":"하종수","userId":"15198303925242776010"}},"outputId":"0ac12de7-1430-447f-91ff-b4466a2971a4"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n","ERROR:root:Internal Python error in the inspect module.\n","Below is the traceback from this internal error.\n","\n"]},{"output_type":"stream","name":"stdout","text":["Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\", line 409, in hf_raise_for_status\n","    response.raise_for_status()\n","  File \"/usr/local/lib/python3.11/dist-packages/requests/models.py\", line 1024, in raise_for_status\n","    raise HTTPError(http_error_msg, response=self)\n","requests.exceptions.HTTPError: 401 Client Error: Unauthorized for url: https://router.huggingface.co/hf-inference/models/microsoft/Phi-3-mini-4k-instruct/v1/chat/completions\n","\n","The above exception was the direct cause of the following exception:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n","    exec(code_obj, self.user_global_ns, self.user_ns)\n","  File \"/tmp/ipython-input-4-264836736.py\", line 10, in <cell line: 0>\n","    chat_model.invoke('프랑스의 수도는 어디인가요?')\n","  File \"/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\", line 372, in invoke\n","    self.generate_prompt(\n","  File \"/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\", line 957, in generate_prompt\n","    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\", line 776, in generate\n","    self._generate_with_cache(\n","  File \"/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\", line 1022, in _generate_with_cache\n","    result = self._generate(\n","             ^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/langchain_huggingface/chat_models/huggingface.py\", line 574, in _generate\n","    answer = self.llm.client.chat_completion(messages=message_dicts, **params)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_client.py\", line 924, in chat_completion\n","    data = self._inner_post(request_parameters, stream=stream)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_client.py\", line 280, in _inner_post\n","    hf_raise_for_status(response)\n","  File \"/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\", line 482, in hf_raise_for_status\n","    raise _format(HfHubHTTPError, str(e), response) from e\n","huggingface_hub.errors.HfHubHTTPError: 401 Client Error: Unauthorized for url: https://router.huggingface.co/hf-inference/models/microsoft/Phi-3-mini-4k-instruct/v1/chat/completions (Request ID: Root=1-685e2b01-4a8a60047fac43c040c5b73b;a5dcb070-6664-4ee3-9e57-9d9ae08ba486)\n","\n","Invalid username or password.\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\", line 2099, in showtraceback\n","    stb = value._render_traceback_()\n","          ^^^^^^^^^^^^^^^^^^^^^^^^\n","AttributeError: 'HfHubHTTPError' object has no attribute '_render_traceback_'\n","\n","During handling of the above exception, another exception occurred:\n","\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 1101, in get_records\n","    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n","           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 248, in wrapped\n","    return f(*args, **kwargs)\n","           ^^^^^^^^^^^^^^^^^^\n","  File \"/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\", line 281, in _fixed_getinnerframes\n","    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n","                                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/inspect.py\", line 1739, in getinnerframes\n","    traceback_info = getframeinfo(tb, context)\n","                     ^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/inspect.py\", line 1684, in getframeinfo\n","    filename = getsourcefile(frame) or getfile(frame)\n","               ^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/inspect.py\", line 948, in getsourcefile\n","    module = getmodule(object, filename)\n","             ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n","  File \"/usr/lib/python3.11/inspect.py\", line 997, in getmodule\n","    os.path.realpath(f)] = module.__name__\n","    ^^^^^^^^^^^^^^^^^^^\n","  File \"<frozen posixpath>\", line 416, in realpath\n","  File \"<frozen posixpath>\", line 460, in _joinrealpath\n","KeyboardInterrupt\n"]},{"output_type":"error","ename":"TypeError","evalue":"object of type 'NoneType' has no len()","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    408\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1023\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1024\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://router.huggingface.co/hf-inference/models/microsoft/Phi-3-mini-4k-instruct/v1/chat/completions","\nThe above exception was the direct cause of the following exception:\n","\u001b[0;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-4-264836736.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mchat_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'프랑스의 수도는 어디인가요?'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, stop, **kwargs)\u001b[0m\n\u001b[1;32m    371\u001b[0m             \u001b[0;34m\"ChatGeneration\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 372\u001b[0;31m             self.generate_prompt(\n\u001b[0m\u001b[1;32m    373\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    956\u001b[0m         \u001b[0mprompt_messages\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprompts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 957\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, messages, stop, callbacks, tags, metadata, run_name, run_id, **kwargs)\u001b[0m\n\u001b[1;32m    775\u001b[0m                 results.append(\n\u001b[0;32m--> 776\u001b[0;31m                     self._generate_with_cache(\n\u001b[0m\u001b[1;32m    777\u001b[0m                         \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_core/language_models/chat_models.py\u001b[0m in \u001b[0;36m_generate_with_cache\u001b[0;34m(self, messages, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1021\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m             result = self._generate(\n\u001b[0m\u001b[1;32m   1023\u001b[0m                 \u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrun_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_huggingface/chat_models/huggingface.py\u001b[0m in \u001b[0;36m_generate\u001b[0;34m(self, messages, stop, run_manager, stream, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m             }\n\u001b[0;32m--> 574\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat_completion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage_dicts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    575\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_chat_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36mchat_completion\u001b[0;34m(self, messages, model, stream, frequency_penalty, logit_bias, logprobs, max_tokens, n, presence_penalty, response_format, seed, stop, stream_options, temperature, tool_choice, tool_prompt, tools, top_logprobs, top_p, extra_body)\u001b[0m\n\u001b[1;32m    923\u001b[0m         )\n\u001b[0;32m--> 924\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inner_post\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_parameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    925\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/inference/_client.py\u001b[0m in \u001b[0;36m_inner_post\u001b[0;34m(self, request_parameters, stream)\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m             \u001b[0mhf_raise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_lines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mstream\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_http.py\u001b[0m in \u001b[0;36mhf_raise_for_status\u001b[0;34m(response, endpoint_name)\u001b[0m\n\u001b[1;32m    481\u001b[0m         \u001b[0;31m# as well (request id and/or server error message)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0m_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mHfHubHTTPError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mHfHubHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://router.huggingface.co/hf-inference/models/microsoft/Phi-3-mini-4k-instruct/v1/chat/completions (Request ID: Root=1-685e2b01-4a8a60047fac43c040c5b73b;a5dcb070-6664-4ee3-9e57-9d9ae08ba486)\n\nInvalid username or password.","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2098\u001b[0m                         \u001b[0;31m# in the engines. This should return a list of strings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2099\u001b[0;31m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mAttributeError\u001b[0m: 'HfHubHTTPError' object has no attribute '_render_traceback_'","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mshowtraceback\u001b[0;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[1;32m   2099\u001b[0m                         \u001b[0mstb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_render_traceback_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2100\u001b[0m                     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2101\u001b[0;31m                         stb = self.InteractiveTB.structured_traceback(etype,\n\u001b[0m\u001b[1;32m   2102\u001b[0m                                             value, tb, tb_offset=tb_offset)\n\u001b[1;32m   2103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1365\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1366\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         return FormattedTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1368\u001b[0m             self, etype, value, tb, tb_offset, number_of_lines_of_context)\n\u001b[1;32m   1369\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1265\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose_modes\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1266\u001b[0m             \u001b[0;31m# Verbose modes need a full traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1267\u001b[0;31m             return VerboseTB.structured_traceback(\n\u001b[0m\u001b[1;32m   1268\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtb_offset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumber_of_lines_of_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1269\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mstructured_traceback\u001b[0;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[1;32m   1122\u001b[0m         \u001b[0;34m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1124\u001b[0;31m         formatted_exception = self.format_exception_as_a_whole(etype, evalue, etb, number_of_lines_of_context,\n\u001b[0m\u001b[1;32m   1125\u001b[0m                                                                tb_offset)\n\u001b[1;32m   1126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mformat_exception_as_a_whole\u001b[0;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[1;32m   1080\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1081\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1082\u001b[0;31m         \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_recursion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_etype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1083\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1084\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_unique\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecursion_repeat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/IPython/core/ultratb.py\u001b[0m in \u001b[0;36mfind_recursion\u001b[0;34m(etype, value, records)\u001b[0m\n\u001b[1;32m    380\u001b[0m     \u001b[0;31m# first frame (from in to out) that looks different.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_recursion_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0metype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrecords\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[0;31m# Select filename, lineno, func_name to track frames with\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: object of type 'NoneType' has no len()"]}]},{"cell_type":"code","source":["from langchain_huggingface import HuggingFacePipeline\n","\n","pipe = HuggingFacePipeline.from_model_id(\n","    model_id = 'microsoft/Phi-3-mini-4k-instruct',\n","    task = 'text-generation',\n",")\n","\n","pipe.invoke('What is LLM')"],"metadata":{"id":"9FtGZh7gyxsF","executionInfo":{"status":"aborted","timestamp":1751001858413,"user_tz":-540,"elapsed":8726,"user":{"displayName":"하종수","userId":"15198303925242776010"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Model Laboratory"],"metadata":{"id":"CpXLxDW21dj8"}},{"cell_type":"code","source":["from langchain.model_laboratory import ModelLaboratory\n","\n","llms = [\n","        ChatOpenAI(model_name= 'gpt-3.5-turbo'),\n","        ChatOpenAI(model_name= 'gpt-4.1')]\n","\n","lab = ModelLaboratory.from_llms(llms)\n","lab.compare('파이썬의 장점이 무엇인가요?')"],"metadata":{"id":"LX6fPJyezrG7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751001882084,"user_tz":-540,"elapsed":12910,"user":{"displayName":"하종수","userId":"15198303925242776010"}},"outputId":"b6664ee9-2ed3-4cb1-da32-08a625136544"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1mInput:\u001b[0m\n","파이썬의 장점이 무엇인가요?\n","\n","client=<openai.resources.chat.completions.completions.Completions object at 0x78662c4eda10> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x78662c4ede10> root_client=<openai.OpenAI object at 0x78662d80e310> root_async_client=<openai.AsyncOpenAI object at 0x78662c4edb90> model_kwargs={} openai_api_key=SecretStr('**********')\n","\u001b[36;1m\u001b[1;3m1. 쉬운 학습 곡선: 파이썬은 간결하고 읽기 쉬운 문법을 가지고 있어 쉽게 학습할 수 있습니다. 이는 프로그래밍 초보자들에게 특히 도움이 됩니다.\n","\n","2. 다양한 응용 분야: 파이썬은 웹 개발, 데이터 분석, 인공지능, 머신러닝, 자연어 처리 등 다양한 분야에서 널리 사용되고 있습니다. 이로 인해 파이썬 개발자는 다양한 분야에서 일을 할 수 있습니다.\n","\n","3. 강력한 라이브러리: 파이썬은 다양한 라이브러리와 프레임워크를 제공하고 있어 개발 속도를 높일 수 있습니다. 예를 들어, NumPy, Pandas, Matplotlib 등 데이터 분석 및 시각화를 위한 라이브러리가 내장되어 있습니다.\n","\n","4. 커뮤니티 지원: 파이썬은 활발한 온라인 커뮤니티를 가지고 있어 개발자들이 서로 정보를 공유하고 도움을 주고 받을 수 있습니다. 또한 다양한 오픈소스 프로젝트로 인해 뛰어난 지원을 받을 수 있습니다.\n","\n","5. 크로스 플랫폼: 파이썬은 윈도우, 맥, 리눅스 등 다양한 운영 체제에서 동작하며, 코드를 한 번 작성하면 어디서든 실행할 수 있습니다. 이는 개발자에게 유연성을 제공합니다.\u001b[0m\n","\n","client=<openai.resources.chat.completions.completions.Completions object at 0x78662c4ee6d0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x78662c4eed50> root_client=<openai.OpenAI object at 0x78662c4ee2d0> root_async_client=<openai.AsyncOpenAI object at 0x78662c4ee790> model_name='gpt-4.1' model_kwargs={} openai_api_key=SecretStr('**********')\n","\u001b[33;1m\u001b[1;3m파이썬(Python)의 주요 장점은 다음과 같습니다:\n","\n","1. **쉬운 문법**: 파이썬은 문법이 간결하고 직관적이어서 초보자도 쉽게 배울 수 있습니다. 코드가 영어 문장처럼 읽히는 경우가 많아 이해하기 쉽습니다.\n","\n","2. **풍부한 라이브러리**: 데이터 분석, 웹 개발, 인공지능, 네트워크 등 다양한 분야에서 활용할 수 있는 라이브러리와 프레임워크(예: NumPy, pandas, TensorFlow, Django 등)가 매우 많습니다.\n","\n","3. **높은 호환성 및 이식성**: 파이썬 코드는 Windows, macOS, Linux 등 여러 운영체제에서 거의 변경 없이 사용할 수 있습니다.\n","\n","4. **큰 커뮤니티**: 사용자가 많아 문제를 해결하거나, 자료를 찾고 질문을 할 수 있는 커뮤니티(예: Stack Overflow, GitHub, 공식 문서 등)가 잘 활성화되어 있습니다.\n","\n","5. **다양한 분야에서 활용**: 웹 개발, 데이터 분석, 인공지능, 머신러닝, 자동화, 크롤링, 제어 시스템 등 다양한 분야에 사용되고 있습니다.\n","\n","6. **유연한 프로그래밍 패러다임**: 객체지향, 절차지향, 함수형 프로그래밍 등 여러 패러다임을 지원합니다.\n","\n","7. **인터프리터 언어**: 별도의 컴파일 과정 없이 코드를 바로 실행할 수 있어, 개발/테스트/디버깅이 편리합니다.\n","\n","8. **생산성 향상**: 위의 다양한 장점 덕분에 빠르게 프로토타입을 만들고 실행해볼 수 있어 개발 생산성이 높습니다.\n","\n","요약하면 파이썬은 배우기 쉽고, 활용 범위가 넓으며, 개발 속도가 빠른 '실용적인' 언어입니다.\u001b[0m\n","\n"]}]},{"cell_type":"markdown","source":["## Prompts\n","https://python.langchain.com/api_reference/core/prompts.html#langchain-core-prompts\n","\n","`LangChain`의 API 문서에서 제공하는 **Prompts**에 대한 내용은 LangChain 프레임워크의 **핵심 구성 요소 중 하나**로, LLM(Large Language Model)과의 인터페이스를 설정하는 데 중요한 역할을 한다. Prompts는 LLM에 전달될 입력을 정의하고, 구조화하며, 이를 기반으로 원하는 응답을 얻기 위해 사용된다.\n","\n","**주요 사용처**\n","\n","1. **자동화된 입력 구성**\n","   - PromptTemplate을 사용하여 사용자 입력을 자동으로 구성.\n","   - 동일한 형식의 질문이나 대화를 대량으로 생성 가능.\n","\n","2. **대화형 응답**\n","   - ChatPromptTemplate을 통해 대화형 AI의 문맥 유지를 지원.\n","\n","3. **샘플 기반 학습**\n","   - Few-shot Prompt는 LLM에 구체적인 예제를 제공해 정확한 응답을 유도.\n","\n","4. **결과 파싱**\n","   - Output Parsers를 통해 LLM의 출력을 특정 포맷으로 처리하여 후속 작업을 자동화.\n","\n","\n","**클래스 계층구조**\n","```\n","BasePromptTemplate\n","├─ PipelinePromptTemplate\n","├─ StringPromptTemplate\n","│  ├─ PromptTemplate\n","│  ├─ FewShotPromptTemplate\n","│  └─ FewShotPromptWithTemplates\n","└─ BaseChatPromptTemplate\n","   ├─ AutoGPTPrompt\n","   └─ ChatPromptTemplate\n","      └─ AgentScratchPadChatPromptTemplate\n","\n","BaseMessagePromptTemplate\n","├─ MessagesPlaceholder\n","└─ BaseStringMessagePromptTemplate\n","   ├─ ChatMessagePromptTemplate\n","   ├─ HumanMessagePromptTemplate\n","   ├─ AIMessagePromptTemplate\n","   └─ SystemMessagePromptTemplate\n","\n","```"],"metadata":{"id":"5p8hjLEn2b0t"}},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","\n","llm = ChatOpenAI(model_name = 'gpt-4o-mini')\n","\n","llm.invoke('LLM이 뭔가요?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RLJm_AVU2Cp2","executionInfo":{"status":"ok","timestamp":1751002280747,"user_tz":-540,"elapsed":2946,"user":{"displayName":"하종수","userId":"15198303925242776010"}},"outputId":"5eb6f7b9-fbe9-4e05-bfbb-e289bd220951"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='LLM은 \"Large Language Model\"의 약자로, 대량의 텍스트 데이터를 학습하여 자연어 처리(NLP) 작업을 수행할 수 있는 인공지능 모델을 의미합니다. 이러한 모델은 텍스트 생성, 질문 응답, 번역, 요약 등 다양한 언어 관련 작업을 처리하는 데 사용됩니다. 예를 들어, OpenAI의 GPT(Generative Pre-trained Transformer) 시리즈와 같은 모델들이 이에 해당합니다.\\n\\nLLM은 많은 양의 데이터와 강력한 계산 능력을 활용하여 언어의 패턴과 구조를 학습하므로, 높은 수준의 언어 이해와 생성을 가능하게 합니다. 이를 통해 사용자와의 대화, 글쓰기 지원, 정보 검색 등 다양한 용도로 활용됩니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 160, 'prompt_tokens': 15, 'total_tokens': 175, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BmvyAokZzIZv0FE11kDkt47Hk9UxL', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--e82e2187-9ce8-4e00-8657-bd1452bcd212-0', usage_metadata={'input_tokens': 15, 'output_tokens': 160, 'total_tokens': 175, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["messages = [\n","    ('system', '당신은 친절한 초딩전용 챗봇입니다. 초딩의 눈높이에 맞게 설명해주세요.'),\n","    ('human', '랭체인이 뭔가요?'),\n","]\n","llm.invoke(messages)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"M5kx6mAQ3j95","executionInfo":{"status":"ok","timestamp":1751002389205,"user_tz":-540,"elapsed":3859,"user":{"displayName":"하종수","userId":"15198303925242776010"}},"outputId":"30b3ab41-423e-4085-d4fa-add1ae7a408a"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='랭체인(RankChain)은 사람들이 정보를 정리하고, 연결하는 방법을 돕는 도구나 시스템이에요. 이걸로 여러 가지 정보를 한 곳에 모아서 더 쉽게 이해하고 사용할 수 있도록 도와줘요. 예를 들어, 책이나 논문 같은 내용을 보다 체계적으로 정리해서 필요한 부분을 쉽게 찾을 수 있게 해 준답니다.\\n\\n구체적으로는, 사람들이 어떤 것들을 중요하게 생각하는지 평가하고, 그에 따라 정보를 순서대로 정리하는 방식이에요. 그래서 만든 정보의 흐름이 더 명확해지고, 찾기도 쉬워진답니다. 예를 들어 친구들과의 대화 내용을 정리할 때, 중요한 내용을 먼저 적어놓거나 카테고리별로 나눌 수 있겠죠. \\n\\n이해가 좀 더 잘 됐나요? 궁금한 점이 있으면 언제든지 물어봐요!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 194, 'prompt_tokens': 47, 'total_tokens': 241, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-Bmvzt8wEYSJB6yTlsqIsyy6AmrESh', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--629d2e44-e5f4-4fdf-a77f-8f4669261652-0', usage_metadata={'input_tokens': 47, 'output_tokens': 194, 'total_tokens': 241, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["### Prompt Template"],"metadata":{"id":"PHKLbM0_4Mqm"}},{"cell_type":"code","source":["from langchain import PromptTemplate\n","\n","# 어떤 상품에 대한 광고문구를 생성.\n","\n","prompt_template = PromptTemplate(\n","    template='{product}를 홍보하기 위한 신박한 광고문구를 작성해줘',\n","    input_variables=[\"product\"]\n",")\n","\n","prompt = prompt_template.format(product='초소형 카메라')\n","\n","ai_message = llm.invoke(prompt)\n","print(ai_message.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JEsQkTId4ArM","executionInfo":{"status":"ok","timestamp":1751002784082,"user_tz":-540,"elapsed":1530,"user":{"displayName":"하종수","userId":"15198303925242776010"}},"outputId":"00a88df4-3339-4b63-9599-7d33cb0351ec"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\"작은 것의 힘, 소소한 순간을 담다! 초소형 카메라와 함께라면 당신의 기억이 언제 어디서나 살아납니다. 주머니 속의 거대한 이야기, 지금 바로 만져보세요!\"\n"]}]},{"cell_type":"markdown","source":["### ChatPromptTemplate"],"metadata":{"id":"0X4ZPFaG5sDo"}},{"cell_type":"code","source":["from langchain.propmts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n","\n","system_msg_template = SystemMessagePromptTemplate.from_template(\"당신은 {domain}분야의 최고의 챗봇입니다.\")\n","human_msg_template = HumanMessagePromptTemplate.from_template(\"{question}\")\n","chat_template = ChatPromptTemplate.fom(messages([system_msg_template, human_msg_template]))\n","\n","prompt = chat_template.format_messages(domain='IT',question='SLM이 뭐야?')\n","\n","llm.invoke(prompt).content"],"metadata":{"id":"OfGOQGR_5DZ4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["domain = '육아'\n","human_msg_template = HumanMessagePromptTemplate.from_template(\"{question}\")\n","chat_template = ChatPromptTemplate.fom(messages([system_msg_template, human_msg_template]))\n","\n","prompt = chat_template.format_messages(domain='육아',question='')\n","\n","llm.invoke(prompt).content"],"metadata":{"id":"oyRidkrp7Vf2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### FewShotPromptTemplate"],"metadata":{"id":"U6oEEL6rCmpb"}},{"cell_type":"code","source":["from langchain.prompts import FewShotPromptTemplate\n","\n","examples = [\n","    {'q' : '2 + 2', 'a' : '4'},\n","    {\"q\" : '3 + 5', 'a' : '8'},\n","]\n","\n","prompt_template = PromptTemplate(\n","    template='Q: {q}\\nA: {a}',\n","    input_variables = ['q','a']\n",")\n","\n","fewshot_template = FewShotPromptTemplate(\n","    examples = examples,\n","    example_prompt = prompt_template,\n","    prefix = '다음 수학문제를 풀어주세요',\n","    suffix = 'Q: {question} \\nA:', # 사용자입력값\n","    input_variables = ['question']\n",")\n","\n","prompt = fewshot_template.format(question = '123 + 345 = ?')\n","print(prompt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1cxHgQNZCpuF","executionInfo":{"status":"ok","timestamp":1751005516978,"user_tz":-540,"elapsed":11,"user":{"displayName":"하종수","userId":"15198303925242776010"}},"outputId":"645a156e-f5a8-460b-e129-fc51e7979d8c"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["다음 수학문제를 풀어주세요\n","\n","Q: 2 + 2\n","A: 4\n","\n","Q: 3 + 5\n","A: 8\n","\n","Q: 123 + 345 = ? \n","A:\n"]}]},{"cell_type":"code","source":["print(llm.invoke(prompt).content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"umrxlnmiD6xk","executionInfo":{"status":"ok","timestamp":1751005578024,"user_tz":-540,"elapsed":778,"user":{"displayName":"하종수","userId":"15198303925242776010"}},"outputId":"70790ff4-e63d-4faf-a25c-fd06caef5982"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["A: 468\n"]}]},{"cell_type":"markdown","source":["## Output Parsers\n","\n","https://python.langchain.com/api_reference/langchain/output_parsers.html#module-langchain.output_parsers\n","\n","LangChain의 Output Parsers는 LLM이 생성한 텍스트 출력을 특정 형식으로 변환하거나 처리하는 데 사용된다. 이는 모델의 응답을 해석하고, 이를 구조화된 데이터로 바꿔 후속 작업에 활용하기 위해 설계되었다. Output Parsers는 LangChain의 응답 처리 워크플로우에서 중요한 역할을 한다.\n","\n","예를 들어, LLM 응답이 \"Name: John, Age: 30\"와 같은 텍스트라면, 이를 {\"name\": \"John\", \"age\": 30}과 같은 Python 딕셔너리로 변환 가능.\n","\n","**사용 목적**\n","- 모델의 출력을 특정 애플리케이션에 맞게 처리해야 하는 경우가 많음.\n","- 응답을 해석하는 일관성과 정확성을 높이기 위해 필요.\n","- 텍스트 기반 응답을 JSON, 리스트 또는 숫자와 같은 특정 포맷으로 변환하여 후속 작업에 활용.\n","\n","**종류**\n","1. **BaseOutputParser**: Output Parsers의 기본 클래스, 커스텀 파서 구현 시 사용.  \n","2. **CommaSeparatedListOutputParser**: 콤마로 구분된 문자열을 리스트로 변환.  \n","3. **RegexParser**: 정규식을 사용해 특정 패턴을 추출하고 키-값 형태로 반환.  \n","4. **StructuredOutputParser**: 출력의 JSON 또는 구조화된 형식을 강제.  \n","5. **PydanticOutputParser**: Pydantic 모델을 기반으로 출력 검증 및 변환.  \n","6. **MarkdownOutputParser**: 마크다운 형식의 텍스트에서 데이터를 추출.  "],"metadata":{"id":"DyGkT0QzEu94"}},{"cell_type":"markdown","source":["### CommaSeparatedListOutputParser"],"metadata":{"id":"XZVexAkqFF65"}},{"cell_type":"code","source":["from langchain.output_parsers import CommaSeparatedListOutputParser\n","\n","model_output = \"사과, 바나나, 오렌지, 포도\"\n","\n","output_parser = CommaSeparatedListOutputParser()\n","\n","output = output_parser.parse(model_output)\n","output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bk0tIlbJEL8j","executionInfo":{"status":"ok","timestamp":1751005912551,"user_tz":-540,"elapsed":22,"user":{"displayName":"하종수","userId":"15198303925242776010"}},"outputId":"4c00f2dc-6e30-4525-ce0c-f69715befa95"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['사과', '바나나', '오렌지', '포도']"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["# {야구}팀 {4}개 질문\n","# {축구}팀 {10}개 질문\n","\n","prompt_template = PromptTemplate(\n","    template=\"{subject} {n}개의 팀을 보여주세요.\\n{format_instruction}\",\n","    input_variables = ['subject', 'n'], # 사용자 프롬프트로 채워질 변수\n","    partial_variables={\n","        'format_instruction' : output_parser.get_format_instructions() # template 생성시에 채워짐\n","    }\n","\n","\n",")\n","\n","prompt = prompt_template.format(subject='대한민국 프로야구', n =5)"],"metadata":{"id":"GOvlqVL1FdzS","executionInfo":{"status":"ok","timestamp":1751006546607,"user_tz":-540,"elapsed":4,"user":{"displayName":"하종수","userId":"15198303925242776010"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["ai_message = llm.invoke(prompt)\n","output = ai_message.content\n","\n","# 출력파서가 가공한 최종출력\n","output = output_parser.parse(output)\n","output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LoPupI2NGt6y","executionInfo":{"status":"ok","timestamp":1751006548239,"user_tz":-540,"elapsed":930,"user":{"displayName":"하종수","userId":"15198303925242776010"}},"outputId":"67f5b1af-a438-4b11-85c2-1defa2b333b6"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['두산 베어스', 'LG 트윈스', '삼성 라이온즈', '키움 히어로즈', 'SSG 랜더스']"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["chain = prompt_template | llm | output_parser\n","chain.invoke(input={'subject':'프로농구', 'n':3})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bQgKET3jH4xx","executionInfo":{"status":"ok","timestamp":1751006797695,"user_tz":-540,"elapsed":604,"user":{"displayName":"하종수","userId":"15198303925242776010"}},"outputId":"e13918f5-70bd-48fc-982c-b38414efc119"},"execution_count":25,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['서울 SK', '전주 KCC', '안양 KGC']"]},"metadata":{},"execution_count":25}]},{"cell_type":"markdown","source":["### JSONOutputParser"],"metadata":{"id":"fVNzr_Z_JJ2c"}},{"cell_type":"code","source":["from langchain_core.output_parsers import JsonOutputParser\n","\n","model_output = '{\"title\": \"GPT-5를 소개합니다.\", \"author\" : \"OpenAI\", \"pages\" : 250}'\n","\n","json_parser = JsonOutputParser()\n","print(json_parser.get_format_instructions())\n","\n","output = json_parser.parse(model_output) # json_str -> python object(list, dict)\n","print(output)\n","print(type(output))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EXALOX_KIXs5","executionInfo":{"status":"ok","timestamp":1751007892331,"user_tz":-540,"elapsed":30,"user":{"displayName":"하종수","userId":"15198303925242776010"}},"outputId":"8b3611b3-db23-4d64-fa3e-2687a23911c2"},"execution_count":28,"outputs":[{"output_type":"stream","name":"stdout","text":["Return a JSON object.\n","{'title': 'GPT-5를 소개합니다.', 'author': 'OpenAI', 'pages': 250}\n","<class 'dict'>\n"]}]},{"cell_type":"code","source":["# AI 관련 책 3권을 보여주세요. (json)\n","# 요리 관련 책 5권을 보여주세요. (json)\n","# PromptTemplate - LLm - output_parser\n","\n","prompt_template = PromptTemplate(\n","    template=\"{subject} 관련 책 {n}권을 추천해주세요.\\n{format_instruction}\",\n","    input_variables = ['subject', 'n'], # 사용자 프롬프트로 채워질 변수\n","    partial_variables={\n","        'format_instruction' : json_parser.get_format_instructions() # template 생성시에 채워짐\n","    }\n",")\n","\n","chain = prompt_template | llm | json_parser\n","print(chain.invoke(input={'subject':'인공지능', 'n':3}))\n","print(chain.invoke(input={'subject':'요리', 'n':5}))\n","# chain.invoke(input={'subject':'AI', 'n':3})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H56nnI2LJqIK","executionInfo":{"status":"ok","timestamp":1751009763379,"user_tz":-540,"elapsed":11040,"user":{"displayName":"하종수","userId":"15198303925242776010"}},"outputId":"2f6cf272-b700-41cc-9366-8262713c937f"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["{'recommended_books': [{'title': 'Artificial Intelligence: A Guide to Intelligent Systems', 'author': 'Michael Negnevitsky', 'description': 'This book provides a comprehensive introduction to the concepts and techniques of artificial intelligence, covering topics such as machine learning, neural networks, and natural language processing.'}, {'title': 'Deep Learning', 'author': 'Ian Goodfellow, Yoshua Bengio, and Aaron Courville', 'description': 'A thorough exploration of deep learning, this book covers fundamental concepts, theoretical foundations, and practical applications, making it suitable for both beginners and experienced practitioners.'}, {'title': 'Artificial Intelligence: Foundations of Computational Agents', 'author': 'David L. Poole and Alan K. Mackworth', 'description': 'This book provides a comprehensive introduction to the principles and practices of AI, focusing on the foundations of computational agents, reasoning, and learning.'}]}\n","{'cookbooks': [{'title': 'The Joy of Cooking', 'author': 'Irma S. Rombauer', 'description': 'A classic cookbook that covers a wide range of recipes and cooking techniques, perfect for beginners and experienced cooks alike.'}, {'title': 'Salt, Fat, Acid, Heat', 'author': 'Samin Nosrat', 'description': 'This book teaches the fundamental elements of cooking that can help anyone become a better chef, blending science with practical advice.'}, {'title': 'The Food Lab: Better Home Cooking Through Science', 'author': 'J. Kenji López-Alt', 'description': 'An exploration of the science behind cooking, offering recipes and techniques that improve results and enhance flavors.'}, {'title': 'How to Cook Everything', 'author': 'Mark Bittman', 'description': 'A comprehensive guide featuring thousands of recipes and cooking techniques, perfect for home cooks who want to master the basics and get creative.'}, {'title': \"Plenty: Vibrant Vegetable Recipes from London's Ottolenghi\", 'author': 'Yotam Ottolenghi', 'description': 'A celebration of vegetable dishes that focuses on bold flavors and unique combinations, catering to both vegetarians and meat-lovers.'}]}\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"uHvoASzjOEwc"},"execution_count":null,"outputs":[]}]}