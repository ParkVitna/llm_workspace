{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOK2R/xnRTQHTcUFjfKZxvM"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"5209d2c3c2304076b87674db8fb389af":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7dc3f2f21453471dbef69c7ad815b226","IPY_MODEL_1581abb7319e4df3a762e9ed4bc7f61f","IPY_MODEL_df9dc685e6d14e408bc847c8476a959d"],"layout":"IPY_MODEL_228261be34cb4a6b91c9552d17bb3f70"}},"7dc3f2f21453471dbef69c7ad815b226":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_94246cc566ef4fd3b2539b5213eebde4","placeholder":"​","style":"IPY_MODEL_4a130275de1f4f6caba5a299684095ba","value":"Loading checkpoint shards:  50%"}},"1581abb7319e4df3a762e9ed4bc7f61f":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_b1438ad3c3a043a4aae7c9eda1168e84","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_1386386e4e33460e99700bfdb0b5242a","value":1}},"df9dc685e6d14e408bc847c8476a959d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_55c9bc63919b43c9aa9677db94af981a","placeholder":"​","style":"IPY_MODEL_ed4ed45550ca40e295b7ed7d0ad8eb2d","value":" 1/2 [00:21&lt;00:21, 21.97s/it]"}},"228261be34cb4a6b91c9552d17bb3f70":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"94246cc566ef4fd3b2539b5213eebde4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a130275de1f4f6caba5a299684095ba":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b1438ad3c3a043a4aae7c9eda1168e84":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1386386e4e33460e99700bfdb0b5242a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"55c9bc63919b43c9aa9677db94af981a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ed4ed45550ca40e295b7ed7d0ad8eb2d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Model IO\n","\n","<img src=\"https://d.pr/i/Wy5B5B+\" width=\"500\"/>\n","\n","- Language Model\n","- Prompt\n","- OutputParser"],"metadata":{"id":"ZScaKInOfmEO"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3cYoEjLufdxA","executionInfo":{"status":"ok","timestamp":1751001709814,"user_tz":-540,"elapsed":11532,"user":{"displayName":"이서","userId":"13175179982189425886"}},"outputId":"2dad4268-d193-4cf2-9bb1-3eb4b707e999"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n","Collecting langchain-openai\n","  Downloading langchain_openai-0.3.26-py3-none-any.whl.metadata (2.3 kB)\n","Collecting langchain-community\n","  Downloading langchain_community-0.3.26-py3-none-any.whl.metadata (2.9 kB)\n","Collecting langchain-huggingface\n","  Downloading langchain_huggingface-0.3.0-py3-none-any.whl.metadata (996 bytes)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.66)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n","Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.1)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.91.0)\n","Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n","Collecting dataclasses-json<0.7,>=0.5.7 (from langchain-community)\n","  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n","Collecting pydantic-settings<3.0.0,>=2.4.0 (from langchain-community)\n","  Downloading pydantic_settings-2.10.1-py3-none-any.whl.metadata (3.4 kB)\n","Collecting httpx-sse<1.0.0,>=0.4.0 (from langchain-community)\n","  Downloading httpx_sse-0.4.1-py3-none-any.whl.metadata (9.4 kB)\n","Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n","Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.1)\n","Requirement already satisfied: huggingface-hub>=0.30.2 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.33.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.5.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n","Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n","Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (2025.3.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (24.2)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (4.67.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (4.14.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (1.1.5)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n","Collecting python-dotenv>=0.21.0 (from pydantic-settings<3.0.0,>=2.4.0->langchain-community)\n","  Downloading python_dotenv-1.1.1-py3-none-any.whl.metadata (24 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.6.15)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n","Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community)\n","  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n","Downloading langchain_openai-0.3.26-py3-none-any.whl (70 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.1/70.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_community-0.3.26-py3-none-any.whl (2.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m82.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading langchain_huggingface-0.3.0-py3-none-any.whl (27 kB)\n","Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n","Downloading httpx_sse-0.4.1-py3-none-any.whl (8.1 kB)\n","Downloading pydantic_settings-2.10.1-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading python_dotenv-1.1.1-py3-none-any.whl (20 kB)\n","Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n","Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n","Installing collected packages: python-dotenv, mypy-extensions, marshmallow, httpx-sse, typing-inspect, pydantic-settings, dataclasses-json, langchain-openai, langchain-huggingface, langchain-community\n","Successfully installed dataclasses-json-0.6.7 httpx-sse-0.4.1 langchain-community-0.3.26 langchain-huggingface-0.3.0 langchain-openai-0.3.26 marshmallow-3.26.1 mypy-extensions-1.1.0 pydantic-settings-2.10.1 python-dotenv-1.1.1 typing-inspect-0.9.0\n"]}],"source":["!pip install langchain langchain-openai langchain-community langchain-huggingface"]},{"cell_type":"code","source":["from google.colab import userdata\n","import os\n","\n","os.environ['LANGSMITH_TRACING'] = userdata.get('LANGSMITH_TRACING')\n","os.environ['LANGSMITH_ENDPOINT'] = userdata.get('LANGSMITH_ENDPOINT')\n","os.environ['LANGSMITH_API_KEY'] = userdata.get('LANGSMITH_API_KEY')\n","os.environ['LANGSMITH_PROJECT'] = userdata.get('LANGSMITH_PROJECT')\n","os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"],"metadata":{"id":"chKp8-7wgCWK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Language Models\n","\n","https://python.langchain.com/api_reference/reference.html#integrations\n","\n","LangChain의 Integrations 섹션에서는 다양한 다운스트림 LLM 모델과의 연동을 지원하다.\n","\n","이 섹션에서는 OpenAI, Hugging Face, GPT-4 등의 다양한 LLM 모델과 LangChain을 연결하는 방법을 다룬다."],"metadata":{"id":"M5Nsx41RgXOk"}},{"cell_type":"markdown","source":["### openai"],"metadata":{"id":"rHhSmtYYxFbq"}},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","\n","llm = ChatOpenAI(model_name='gpt-4o')\n","\n","llm.invoke('태국의 수도는 어디인가요?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3w7GPIYGxNUJ","executionInfo":{"status":"ok","timestamp":1751002053041,"user_tz":-540,"elapsed":2357,"user":{"displayName":"이서","userId":"13175179982189425886"}},"outputId":"8dc59955-1519-45a7-fa23-b74548364715"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='태국의 수도는 방콕입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 16, 'total_tokens': 26, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_07871e2ad8', 'id': 'chatcmpl-BmvuWKnEeolW0ZwPD2LEeoSTx1Tgq', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--2e5a3d51-8fe8-436b-8257-df3d4e83fb47-0', usage_metadata={'input_tokens': 16, 'output_tokens': 10, 'total_tokens': 26, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["### huggingface"],"metadata":{"id":"UMvr5nkUyLQJ"}},{"cell_type":"code","source":["from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n","\n","llm = HuggingFaceEndpoint(\n","    repo_id='microsoft/Phi-3-mini-4k-instruct',\n","    task='text-generation'\n",")\n","\n","chat_model = ChatHuggingFace(\n","    llm=llm,\n","    verbose=True\n",")\n","\n","chat_model.invoke('Where is the capital of France?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oQMTF6SjyOgs","executionInfo":{"status":"ok","timestamp":1751002059740,"user_tz":-540,"elapsed":5190,"user":{"displayName":"이서","userId":"13175179982189425886"}},"outputId":"dca02e07-2fe0-425d-b2df-f6b7495d87a9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='The capital of France is Paris. It is not only the largest city in the country but also one of the most iconic, famous for landmarks such as the Eiffel Tower, Notre-Dame Cathedral, and the Louvre Museum, which houses thousands of works of art, including the Mona Lisa. Paris is also regarded as a global center for art, fashion, gastronomy, and culture. The city is divided into 20 administrative districts called arrondissements, which are numbered from 1 to 20 in a clockwise spiral from the Concorde area on the Right Bank.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 130, 'prompt_tokens': 10, 'total_tokens': 140}, 'model_name': 'microsoft/Phi-3-mini-4k-instruct', 'system_fingerprint': '3.2.1-sha-4d28897', 'finish_reason': 'stop', 'logprobs': None}, id='run--9b5ba5d1-0a34-4180-8514-71bdda521c86-0', usage_metadata={'input_tokens': 10, 'output_tokens': 130, 'total_tokens': 140})"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["from langchain_huggingface import HuggingFacePipeline\n","\n","pipe = HuggingFacePipeline.from_model_id(\n","    model_id='microsoft/Phi-3-mini-4k-instruct',\n","    task='text-generation'\n",")\n","pipe.invoke('What is LLM?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["5209d2c3c2304076b87674db8fb389af","7dc3f2f21453471dbef69c7ad815b226","1581abb7319e4df3a762e9ed4bc7f61f","df9dc685e6d14e408bc847c8476a959d","228261be34cb4a6b91c9552d17bb3f70","94246cc566ef4fd3b2539b5213eebde4","4a130275de1f4f6caba5a299684095ba","b1438ad3c3a043a4aae7c9eda1168e84","1386386e4e33460e99700bfdb0b5242a","55c9bc63919b43c9aa9677db94af981a","ed4ed45550ca40e295b7ed7d0ad8eb2d"]},"id":"bNWBepaTzQ2g","outputId":"21097a59-9f22-47b2-986e-f8d6fd84b4f1"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5209d2c3c2304076b87674db8fb389af"}},"metadata":{}}]},{"cell_type":"markdown","source":["### ModelLaboratory\n","- 여러LLM을 동시에 비교할 수 있는 실험도구"],"metadata":{"id":"HiK9-TJq1U2R"}},{"cell_type":"code","source":["from langchain.model_laboratory import ModelLaboratory\n","\n","llms = [\n","    ChatOpenAI(model_name='gpt-3.5-turbo'),\n","    ChatOpenAI(model_name='gpt-4.1'),\n","]\n","\n","lab = ModelLaboratory.from_llms(llms)\n","lab.compare('파이썬의 장점이 무엇인가요?')"],"metadata":{"id":"1UFg11D_1e7q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751002070859,"user_tz":-540,"elapsed":9369,"user":{"displayName":"이서","userId":"13175179982189425886"}},"outputId":"28c02eac-e5a3-4abe-bfcb-8698af6c23d5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1mInput:\u001b[0m\n","파이썬의 장점이 무엇인가요?\n","\n","client=<openai.resources.chat.completions.completions.Completions object at 0x7da4cc16ba90> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7da4cc171090> root_client=<openai.OpenAI object at 0x7da4cc115910> root_async_client=<openai.AsyncOpenAI object at 0x7da4cc16a7d0> model_kwargs={} openai_api_key=SecretStr('**********')\n","\u001b[36;1m\u001b[1;3m1. 쉽고 간단한 문법: 파이썬은 다른 프로그래밍 언어에 비해 문법이 간단하고 쉽게 이해할 수 있어 입문자들에게 적합하다.\n","\n","2. 다양한 라이브러리: 파이썬은 다양한 라이브러리와 모듈들을 제공하고 있어, 개발자들이 원하는 기능을 빠르게 구현할 수 있다.\n","\n","3. 크로스 플랫폼: 파이썬은 윈도우, 맥, 리눅스 등 다양한 운영체제에서 동일한 코드를 사용할 수 있어 개발이 간편하다.\n","\n","4. 데이터 분석 및 인공지능: 파이썬은 데이터 분석 및 머신러닝, 딥러닝 분야에서 널리 사용되는 언어로, 데이터와 인공지능을 다루는데 매우 효과적이다.\n","\n","5. 커뮤니티 및 생태계: 파이썬은 활발한 커뮤니티와 다양한 오픈소스 프로젝트들이 존재하여, 개발자들이 필요한 정보를 쉽게 얻을 수 있고, 서로 협력하여 발전시킬 수 있다. \n","\n","이러한 이유로 파이썬은 다양한 분야에서 널리 사용되며, 학습과 개발이 쉽고 효율적하다고 할 수 있습니다.\u001b[0m\n","\n","client=<openai.resources.chat.completions.completions.Completions object at 0x7da4cc170c10> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7da4cc15d690> root_client=<openai.OpenAI object at 0x7da4cc172e50> root_async_client=<openai.AsyncOpenAI object at 0x7da4cc15d610> model_name='gpt-4.1' model_kwargs={} openai_api_key=SecretStr('**********')\n","\u001b[33;1m\u001b[1;3m파이썬(Python)의 장점은 다음과 같습니다:\n","\n","1. **문법이 간단하고 읽기 쉬움**  \n","   영어와 비슷한 간결한 문법을 사용하여 초보자도 쉽게 배울 수 있습니다.\n","\n","2. **방대한 라이브러리**  \n","   수치 계산, 인공지능, 데이터 분석, 웹 개발 등 다양한 분야의 라이브러리와 프레임워크가 풍부하여 빠른 개발이 가능합니다. (예: NumPy, pandas, TensorFlow, Django 등)\n","\n","3. **플랫폼 독립적**  \n","   Windows, Mac, Linux 등 다양한 운영체제에서 동일한 코드를 실행할 수 있습니다.\n","\n","4. **커뮤니티와 자료가 풍부함**  \n","   전 세계적으로 사용자가 많아, 질문과 답변, 예제 코드 등 자료를 쉽게 찾을 수 있습니다.\n","\n","5. **생산성 향상**  \n","   짧은 코드로 원하는 기능을 구현할 수 있어 개발 속도가 빠릅니다.\n","\n","6. **확장성과 통합성**  \n","   다른 언어(C, C++, Java 등)와 연동이 가능하며, 다양한 소프트웨어와 쉽게 통합할 수 있습니다.\n","\n","7. **자동 메모리 관리**  \n","   가비지 컬렉션 등 메모리 관리가 자동으로 이루어집니다.\n","\n","8. **스크립트 언어 및 인터프리터 언어**  \n","   컴파일 과정 없이 바로 실행할 수 있어, 프로토타이핑, 실험, 자동화에 적합합니다.\n","\n","**요약:**  \n","파이썬은 쉽고, 강력하며, 다양한 분야에 사용할 수 있는 현대적인 프로그래밍 언어입니다. 초보자뿐만 아니라 전문가에게도 매우 인기 있는 언어입니다.\u001b[0m\n","\n"]}]},{"cell_type":"markdown","source":["## Prompts\n","\n","https://python.langchain.com/api_reference/core/prompts.html#langchain-core-prompts\n","\n","`LangChain`의 API 문서에서 제공하는 **Prompts**에 대한 내용은 LangChain 프레임워크의 **핵심 구성 요소 중 하나**로, LLM(Large Language Model)과의 인터페이스를 설정하는 데 중요한 역할을 한다. Prompts는 LLM에 전달될 입력을 정의하고, 구조화하며, 이를 기반으로 원하는 응답을 얻기 위해 사용된다.\n","\n","**주요 사용처**\n","\n","1. **자동화된 입력 구성**\n","   - PromptTemplate을 사용하여 사용자 입력을 자동으로 구성.\n","   - 동일한 형식의 질문이나 대화를 대량으로 생성 가능.\n","\n","2. **대화형 응답**\n","   - ChatPromptTemplate을 통해 대화형 AI의 문맥 유지를 지원.\n","\n","3. **샘플 기반 학습**\n","   - Few-shot Prompt는 LLM에 구체적인 예제를 제공해 정확한 응답을 유도.\n","\n","4. **결과 파싱**\n","   - Output Parsers를 통해 LLM의 출력을 특정 포맷으로 처리하여 후속 작업을 자동화.\n","\n","\n","**클래스 계층구조**\n","```\n","BasePromptTemplate\n","├─ PipelinePromptTemplate\n","├─ StringPromptTemplate\n","│  ├─ PromptTemplate\n","│  ├─ FewShotPromptTemplate\n","│  └─ FewShotPromptWithTemplates\n","└─ BaseChatPromptTemplate\n","   ├─ AutoGPTPrompt\n","   └─ ChatPromptTemplate\n","      └─ AgentScratchPadChatPromptTemplate\n","\n","BaseMessagePromptTemplate\n","├─ MessagesPlaceholder\n","└─ BaseStringMessagePromptTemplate\n","   ├─ ChatMessagePromptTemplate\n","   ├─ HumanMessagePromptTemplate\n","   ├─ AIMessagePromptTemplate\n","   └─ SystemMessagePromptTemplate\n","\n","```"],"metadata":{"id":"FL2iUpT12dlS"}},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","\n","llm = ChatOpenAI(model_name='gpt-4o-mini')\n","\n","llm.invoke('LLM이 뭔가요?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8t-Jknh62Q00","executionInfo":{"status":"ok","timestamp":1751002303691,"user_tz":-540,"elapsed":3522,"user":{"displayName":"이서","userId":"13175179982189425886"}},"outputId":"a6f330dc-8b27-4e36-fbbd-d33fbee83c82"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='LLM은 \"Large Language Model\"의 약자로, 대규모 언어 모델을 의미합니다. 이러한 모델은 방대한 양의 텍스트 데이터를 학습하여 자연어 처리(NLP) 작업을 수행할 수 있게 설계되었습니다. LLM은 질문에 답하거나, 텍스트 생성, 요약, 번역, 감정 분석 등 다양한 언어 관련 작업을 수행할 수 있습니다. \\n\\n가장 유명한 LLM으로는 OpenAI의 GPT 시리즈, 구글의 BERT, 메타의 LLaMA 등이 있습니다. 이러한 모델들은 기계 학습 기법인 딥러닝을 사용하여 언어의 패턴과 의미를 이해하고 생성할 수 있도록 최적화되어 있습니다. LLM은 대화형 AI, 콘텐츠 생성, 코드 작성 등 다양한 분야에서 활용되고 있습니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 175, 'prompt_tokens': 15, 'total_tokens': 190, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BmvyWtk0QZSADSe5QrRGuhy3v38lJ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--90c94a39-f5b8-4b3b-bff6-5f8c6aa8b437-0', usage_metadata={'input_tokens': 15, 'output_tokens': 175, 'total_tokens': 190, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["messages = [\n","            ('system', '당신은 친절한 초딩전용 챗봇입니다. 초딩의 눈높이에 맞게 설명해 주세요.'),\n","            ('human', '랭체인이 뭔가요?')\n","]\n","llm.invoke(messages)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PJ9zXfM03nfn","executionInfo":{"status":"ok","timestamp":1751002406235,"user_tz":-540,"elapsed":3869,"user":{"displayName":"이서","userId":"13175179982189425886"}},"outputId":"b2a43a2a-ca12-4db5-b665-5837dfd138b7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='랭체인은 \"Language\"와 \"Chain\"을 합친 말로, 주로 AI나 자연어 처리를 사용할 때 쓰이는 기술이에요. 간단히 말하면, 컴퓨터가 사람의 말을 이해하고 대답할 수 있게 도와주는 도구라고 생각하면 돼요.\\n\\n예를 들어, 랭체인을 사용하면 여러 가지 정보를 연결해서 더 똑똑하게 대답할 수 있게 돼요. 이렇게 연결된 정보를 바탕으로 질문에 대한 더 좋은 답변을 만들어 내는 거죠!\\n\\n쉽게 말해서, 랭체인은 컴퓨터가 내 말을 듣고 더 잘 이해해서 나에게 필요한 정보를 주는 방법이라고 할 수 있어요!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 148, 'prompt_tokens': 48, 'total_tokens': 196, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-Bmw0Aj948WfESy9Yja0BuczCxRfsu', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--12b2478a-6f87-489b-a785-efc23b741753-0', usage_metadata={'input_tokens': 48, 'output_tokens': 148, 'total_tokens': 196, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"]},"metadata":{},"execution_count":10}]},{"cell_type":"markdown","source":["### PromptTemplate"],"metadata":{"id":"7ZqZ9JDD4MlF"}},{"cell_type":"code","source":["from langchain import PromptTemplate\n","\n","# 어떤 상품에 대한 광고문구를 생성\n","\n","prompt_template = PromptTemplate(\n","    template='{product}를 홍보하기 위한 신박한 광고문구를 작성해줘',\n","    input_variables=['product']\n",")\n","\n","prompt_template # PromptTemplate 객체 -> prompt로 만들기\n","prompt = prompt_template.format(product='초소형 카메라')\n","prompt\n","\n","ai_message = llm.invoke(prompt)\n","print(ai_message.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b5ar3KVJ4Bcx","executionInfo":{"status":"ok","timestamp":1751002782217,"user_tz":-540,"elapsed":4603,"user":{"displayName":"이서","userId":"13175179982189425886"}},"outputId":"9eda948f-18c8-453a-8ffb-7efe7d228719"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1. \"작지만 강력한, 당신의 순간을 포착하세요! 초소형 카메라와 함께라면 모든 순간이 특별해집니다.\"\n","\n","2. \"세상의 모든 아름다움을 주머니에! 초소형 카메라로 당신의 시각을 넓히세요.\"\n","\n","3. \"작은 크기, 큰 감동! 초소형 카메라로 일상을 예술로 만들어보세요.\"\n","\n","4. \"어디서나 슬쩍, 하지만 확실하게! 초소형 카메라와 함께라면 촬영이 즐거워집니다.\"\n","\n","5. \"당신의 아이디어를 작은 프레임에 담다! 초소형 카메라로 창의력을 펼쳐보세요.\"\n","\n","6. \"크기는 작아도, 품질은 대박! 세상을 담는 새로운 방식을 경험하세요.\"\n","\n","이런 문구들이 초소형 카메라의 매력을 잘 전달할 수 있을 것입니다!\n"]}]},{"cell_type":"markdown","source":["### ChatPromptTemplate"],"metadata":{"id":"Q9cM9JhP5tGH"}},{"cell_type":"code","source":["from langchain.prompts.chat import ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate\n","\n","system_msg_template = SystemMessagePromptTemplate.from_template('당신은 {domain}분야에서 최고의 챗봇입니다.')\n","human_msg_template = HumanMessagePromptTemplate.from_template('{question}')\n","chat_template = ChatPromptTemplate.from_messages([system_msg_template, human_msg_template])\n","\n","prompt = chat_template.format_messages(domain='인공지능', question='LLM이 뭔가요?')\n","prompt\n","\n","llm.invoke(prompt).content # llm 호출"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":71},"id":"bzfL5N--48Q2","executionInfo":{"status":"ok","timestamp":1751003425031,"user_tz":-540,"elapsed":4324,"user":{"displayName":"이서","userId":"13175179982189425886"}},"outputId":"70ff4118-166e-42a4-f4f0-3832d29b3991"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'LLM은 \"Large Language Model\"의 약자로, 대규모 언어 모델을 의미합니다. 이러한 모델은 많은 양의 텍스트 데이터를 기반으로 훈련되어 자연어를 이해하고 생성하는 능력을 갖추고 있습니다. LLM은 다양한 언어적 작업, 예를 들어 텍스트 생성, 요약, 번역, 질문 답변 등을 수행할 수 있습니다. GPT-3, BERT, T5와 같은 모델들이 LLM의 예로 자주 언급됩니다. 이들 모델은 깊은 신경망 구조를 이용해 언어의 패턴과 구조를 학습하여 인간과 유사한 방식으로 언어를 처리할 수 있습니다.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["prompt = chat_template.format_messages(domain='육아', question='우리 아이가 밥을 잘 안먹어요.')\n","print(llm.invoke(prompt).content) # llm 호출"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rBc8Uqpn649Q","executionInfo":{"status":"ok","timestamp":1751003312837,"user_tz":-540,"elapsed":5982,"user":{"displayName":"이서","userId":"13175179982189425886"}},"outputId":"ba8c637f-6efb-492d-ec4c-ce26e745d729"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["아이의 식사가 잘 이루어지지 않는 것은 많은 부모님들이 겪는 고민입니다. 몇 가지 방법을 시도해 보실 수 있습니다:\n","\n","1. **식사 환경 조성**: 식사 시간에는 방해 요소를 없애고, 편안한 분위기를 만들어 주세요. 가족이 함께 식사하는 것도 좋습니다.\n","\n","2. **다양한 음식 시도**: 여러 가지 음식을 제공해 보세요. 색깔이 화려하거나 아이가 좋아할 만한 모양으로 음식을 준비하면 관심을 끌 수 있습니다.\n","\n","3. **아이의 취향 알고 있기**: 아이가 좋아하는 음식이 무엇인지 관찰하고, 그 음식에 새로운 재료를 조금씩 추가해 보세요.\n","\n","4. **적은 양으로 시작하기**: 한 번에 많은 양을 주기보다는 소량으로 제공하고, 아이가 원하는 만큼만 먹게 해주세요.\n","\n","5. **일관된 식사 시간**: 규칙적인 식사 시간을 정하고, 간식은 정해진 시간에만 제공하여 배고픔을 느끼게 하세요.\n","\n","6. **모방의 힘**: 부모가 식사하는 모습을 보여주면, 아이가 자연스럽게 따라할 수 있습니다. 가족이 같은 음식을 함께 먹는 것이 중요한 예시가 됩니다.\n","\n","7. **강요하지 않기**: 강제로 먹이기보다는 긍정적인 피드백을 주고, 아이가 스스로 먹고 싶어 하도록 유도하세요.\n","\n","아이의 식사는 성장과 발달에 매우 중요한 부분이니, 인내심을 가지고 접근하는 것이 중요합니다. 변화를 관찰하는 데는 시간이 필요할 수 있습니다. 그래도 여전히 걱정이 된다면 소아과 전문의와 상담해 보시는 것도 좋은 방법입니다.\n"]}]},{"cell_type":"markdown","source":["## FewShotPromptTemplate"],"metadata":{"id":"8Z64nsR8CmxK"}},{"cell_type":"code","source":["from langchain.prompts import FewShotPromptTemplate\n","\n","examples = [\n","    {'q': '2 + 2 = ?', 'a': '4'},\n","    {'q': '3 + 5 = ?', 'a': '8'},\n","]\n","\n","prompt_template = PromptTemplate(\n","    template='Q: {q}\\nA: {a}',\n","    input_variables=['q', 'a']\n",")\n","\n","fewshot_template = FewShotPromptTemplate(\n","    examples=examples,\n","    example_prompt=prompt_template,\n","    prefix='다음 수학문제를 풀어주세요:',\n","    suffix='Q: {question} \\nA:', # 사용자입력값\n","    input_variables=['question']\n",")\n","\n","prompt = fewshot_template.format(question='123 + 345 = ?')\n","print(prompt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fMaqo1nqEQ1I","executionInfo":{"status":"ok","timestamp":1751005661657,"user_tz":-540,"elapsed":13,"user":{"displayName":"이서","userId":"13175179982189425886"}},"outputId":"4e6a1ce6-6c0e-40ed-cc80-70b622f32e4a"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["다음 수학문제를 풀어주세요:\n","\n","Q: 2 + 2 = ?\n","A: 4\n","\n","Q: 3 + 5 = ?\n","A: 8\n","\n","Q: 123 + 345 = ? \n","A:\n"]}]},{"cell_type":"code","source":["from langchain.prompts import FewShotPromptTemplate\n","\n","examples = [\n","    {'q': '2 + 2 = ?', 'a': '4'},\n","    {'q': '3 + 5 = ?', 'a': '8'},\n","]\n","\n","prompt_template = PromptTemplate(\n","    template='Q: {q}\\nA: {a}',\n","    input_variables=['q', 'a']\n",")\n","\n","fewshot_template = FewShotPromptTemplate(\n","    examples=examples,\n","    example_prompt=prompt_template,\n","    prefix='다음 수학문제를 풀어주세요:',\n","    suffix='Q: {question}\\nA:', # 사용자입력값\n","    input_variables=['question']\n",")\n","\n","prompt = fewshot_template.format(question='123 + 345 = ?')\n","print(prompt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"utYj6RwdCpeV","executionInfo":{"status":"ok","timestamp":1751005898297,"user_tz":-540,"elapsed":10,"user":{"displayName":"이서","userId":"13175179982189425886"}},"outputId":"3732db7b-25f5-4d14-a97f-e2e11f11a501"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["다음 수학문제를 풀어주세요:\n","\n","Q: 2 + 2 = ?\n","A: 4\n","\n","Q: 3 + 5 = ?\n","A: 8\n","\n","Q: 123 + 345 = ?\n","A:\n"]}]},{"cell_type":"code","source":["print(llm.invoke(prompt).content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5vN8Xi5zD5H2","executionInfo":{"status":"ok","timestamp":1751005927330,"user_tz":-540,"elapsed":632,"user":{"displayName":"이서","userId":"13175179982189425886"}},"outputId":"0418af8a-9ca7-4258-8918-0ccfe9674197"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stdout","text":["A: 468\n"]}]},{"cell_type":"markdown","source":["## Output Parsers\n","\n","https://python.langchain.com/api_reference/langchain/output_parsers.html#module-langchain.output_parsers\n","\n","LangChain의 Output Parsers는 LLM이 생성한 텍스트 출력을 특정 형식으로 변환하거나 처리하는 데 사용된다. 이는 모델의 응답을 해석하고, 이를 구조화된 데이터로 바꿔 후속 작업에 활용하기 위해 설계되었다. Output Parsers는 LangChain의 응답 처리 워크플로우에서 중요한 역할을 한다.\n","\n","예를 들어, LLM 응답이 \"Name: John, Age: 30\"와 같은 텍스트라면, 이를 {\"name\": \"John\", \"age\": 30}과 같은 Python 딕셔너리로 변환 가능.\n","\n","**사용 목적**\n","- 모델의 출력을 특정 애플리케이션에 맞게 처리해야 하는 경우가 많음.\n","- 응답을 해석하는 일관성과 정확성을 높이기 위해 필요.\n","- 텍스트 기반 응답을 JSON, 리스트 또는 숫자와 같은 특정 포맷으로 변환하여 후속 작업에 활용.\n","\n","**종류**\n","1. **BaseOutputParser**: Output Parsers의 기본 클래스, 커스텀 파서 구현 시 사용.  \n","2. **CommaSeparatedListOutputParser**: 콤마로 구분된 문자열을 리스트로 변환.  \n","3. **RegexParser**: 정규식을 사용해 특정 패턴을 추출하고 키-값 형태로 반환.  \n","4. **StructuredOutputParser**: 출력의 JSON 또는 구조화된 형식을 강제.  \n","5. **PydanticOutputParser**: Pydantic 모델을 기반으로 출력 검증 및 변환.  \n","6. **MarkdownOutputParser**: 마크다운 형식의 텍스트에서 데이터를 추출.  "],"metadata":{"id":"Fm56W2OiEq_h"}},{"cell_type":"markdown","source":["### CommaSeparatedListOutputParser"],"metadata":{"id":"gKuYg7NsFA-t"}},{"cell_type":"code","source":["from langchain.output_parsers import CommaSeparatedListOutputParser\n","\n","model_output = \"사과, 바나나, 오렌지, 포도\"\n","\n","output_parser = CommaSeparatedListOutputParser()\n","output = output_parser.parse(model_output)\n","output  # 쉼표로 구분된 문자열을 파이썬 리스트로 변환 -> LLM이 나열한 결과를 정제된 리스트로 사용할 때 유용"],"metadata":{"id":"F9jtvxfHFGmD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751005972898,"user_tz":-540,"elapsed":67,"user":{"displayName":"이서","userId":"13175179982189425886"}},"outputId":"2fd35de2-1623-4f40-fd92-deb747d338c8"},"execution_count":43,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['사과', '바나나', '오렌지', '포도']"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["# {야구}팀 {5}개 질문\n","# {축구}팀 {10}개 질문\n","\n","prompt_template = PromptTemplate(\n","    template='{subject} {n}개의 팀을 보여주세요. \\n{format_instruction}',\n","    input_variables=['subject', 'n'], # 사용자 프롬프트로 채워질 변수\n","    partial_variables={\n","        # template 생성시에 채워짐\n","        'format_instruction': output_parser.get_format_instructions()\n","    }\n",")\n","prompt_template\n","\n","prompt = prompt_template.format(subject='대한민국 프로야구', n=5)\n","prompt = prompt_template.format(subject='프리미어리그', n=5)\n","prompt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"2WkbyqHjFtNv","executionInfo":{"status":"ok","timestamp":1751006488589,"user_tz":-540,"elapsed":15,"user":{"displayName":"이서","userId":"13175179982189425886"}},"outputId":"d0373e45-8100-4492-ac85-9eb3b0909caa"},"execution_count":46,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'프리미어리그 5개의 팀을 보여주세요. \\nYour response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["ai_message = llm.invoke(prompt)\n","output = ai_message.content\n","\n","# 출력파서가 가공한 최종출력\n","output = output_parser.parse(output) # ai_message를 리스트 형태로 출력\n","output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Bc1inX6EGvUR","executionInfo":{"status":"ok","timestamp":1751006531614,"user_tz":-540,"elapsed":933,"user":{"displayName":"이서","userId":"13175179982189425886"}},"outputId":"75177129-7b95-4e54-c522-8f33de35dd70"},"execution_count":47,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['맨체스터 시티', '리버풀', '맨체스터 유나이티드', '첼시', '아센널']"]},"metadata":{},"execution_count":47}]},{"cell_type":"code","source":["# | : chain 연산자\n","chain = prompt_template | llm | output_parser # prompt_template 출력이 llm의 입력, llm 출력이 output_parser의 입력\n","chain.invoke(input={'subject':'프로농구', 'n':3}) # invoke(): 체인을 실행하고 최종 결과를 리턴"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"r4Bl0zs3H0uN","executionInfo":{"status":"ok","timestamp":1751006810902,"user_tz":-540,"elapsed":908,"user":{"displayName":"이서","userId":"13175179982189425886"}},"outputId":"c355b844-b6d9-4fdc-dab8-50d4e9f75f08"},"execution_count":52,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['서울 삼성', '부산 KT', '전주 KCC']"]},"metadata":{},"execution_count":52}]},{"cell_type":"markdown","source":["### JSONOutputParser"],"metadata":{"id":"jibeH-HbJChc"}},{"cell_type":"code","source":["from langchain_core.output_parsers import JsonOutputParser\n","\n","model_output = '{\"title\": \"GPT-5를 소개합니다.\", \"author\": \"OpenAI\", \"pages\": 250}'\n","\n","json_parser = JsonOutputParser()\n","json_parser.get_format_instructions() # 출력 시 JSON 객체로 주세요. -> prompt로 사용가능\n","\n","output = json_parser.parse(model_output) # json_str -> python object(list, dict)\n","print(output)\n","print(type(output)) # dict"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Jki08PvUIWb2","executionInfo":{"status":"ok","timestamp":1751007924342,"user_tz":-540,"elapsed":17,"user":{"displayName":"이서","userId":"13175179982189425886"}},"outputId":"861691b5-7b32-4318-c449-c6ca7426cd69"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["{'title': 'GPT-5를 소개합니다.', 'author': 'OpenAI', 'pages': 250}\n","<class 'dict'>\n"]}]},{"cell_type":"code","source":["# AI 관련 책 3권을 보여주세요. (json)\n","# 요리 관련 책 5권을 보여주세요. (json)\n","# PromptTemplate - LLM - JsonOutputParser\n","\n","\n","def langchain(subject: str, n: int):\n","  prompt_template = PromptTemplate(\n","      template='{subject}관련 책 {n}권을 보여주세요. \\n{format_instruction}',\n","      input_variables=['subject', 'n'], # 사용자 프롬프트로 채워질 변수\n","      partial_variables={\n","          # template 생성시에 채워짐\n","          'format_instruction': output_parser.get_format_instructions()\n","      }\n","  )\n","\n","  prompt = prompt_template.format(subject='AI', n=3)\n","  prompt = prompt_template.format(subject='요리', n=5)\n","\n","  ai_message = llm.invoke(prompt)\n","  output = ai_message.content\n","\n","  # 출력파서가 가공한 최종출력\n","  output = output_parser.parse(output) # ai_message를 리스트 형태로 출력\n","\n","  result = chain.invoke(input={'subject':subject, 'n':n})\n","  return result\n","\n","\n","books1 = langchain(\"AI\", 3)\n","books2 = langchain(\"요리\", 5)\n","print(books1)\n","print(books2)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qBPkwdv1Jqjf","executionInfo":{"status":"ok","timestamp":1751008649482,"user_tz":-540,"elapsed":3492,"user":{"displayName":"이서","userId":"13175179982189425886"}},"outputId":"af413425-65b7-44fd-a509-11e3e7c39a35"},"execution_count":61,"outputs":[{"output_type":"stream","name":"stdout","text":["['인공지능: 현대 접근 방식', 'Deep Learning', '인공지능의 모든 것']\n","['안정환의 한식', '백종원의 30분 초간단 요리', '이연복의 중화 요리', '류수영의 요리 비책', '김유진의 비건 요리']\n"]}]},{"cell_type":"code","source":["json_parser = JsonOutputParser()\n","\n","# 환각 주의!!!\n","prompt_template = PromptTemplate(\n","    template='{subject}관련 국내 도서 {n}권을 보여주세요. \\n{format_instruction}',\n","    input_variables=['subject', 'n'], # 사용자 프롬프트로 채워질 변수\n","    partial_variables={\n","        # template 생성시에 채워짐\n","        'format_instruction': json_parser.get_format_instructions()\n","    }\n",")\n","\n","llm = ChatOpenAI(model_name='gpt-4o-mini')\n","\n","prompt = prompt_template.format(subject='AI', n=3)\n","prompt = prompt_template.format(subject='요리', n=5)\n","\n","ai_message = llm.invoke(prompt)\n","output = ai_message.content\n","\n","# 출력파서가 가공한 최종출력\n","output = json_parser.parse(output) # ai_message를 리스트 형태로 출력\n","\n","chain = prompt_template | llm | json_parser # prompt_template 출력이 llm의 입력, llm 출력이 output_parser의 입력\n","chain.invoke(input={'subject':'AI', 'n':3}) # invoke(): 체인을 실행하고 최종 결과를 리턴\n","chain.invoke(input={'subject':'요리', 'n':5})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dEdR-vrCNzFL","executionInfo":{"status":"ok","timestamp":1751010079627,"user_tz":-540,"elapsed":15066,"user":{"displayName":"이서","userId":"13175179982189425886"}},"outputId":"0d73fb60-3e97-4ba4-cf48-1bbf0e0ec2fd"},"execution_count":65,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'cooking_books': [{'title': '맛있는 식탁',\n","   'author': '오세득',\n","   'publisher': '더클래식',\n","   'publication_year': 2021,\n","   'description': '한국의 전통 요리를 현대적으로 재해석한 요리책.'},\n","  {'title': '파스타, 피자, 그리고 이탈리아',\n","   'author': '강지영',\n","   'publisher': '한빛미디어',\n","   'publication_year': 2020,\n","   'description': '이탈리아 요리의 기본인 파스타와 피자의 다양한 레시피.'},\n","  {'title': '채식의 힘',\n","   'author': '손정현',\n","   'publisher': '반짝이는책',\n","   'publication_year': 2022,\n","   'description': '새로운 채식 레시피와 건강한 식생활을 제안하는 책.'},\n","  {'title': '집에서 즐기는 일식',\n","   'author': '이주현',\n","   'publisher': '예쁜글씨',\n","   'publication_year': 2019,\n","   'description': '집에서 쉽게 만들 수 있는 일본 가정식 레시피.'},\n","  {'title': '홈베이킹',\n","   'author': '이유리',\n","   'publisher': '북아띠',\n","   'publication_year': 2021,\n","   'description': '초보자도 쉽게 따라할 수 있는 다양한 베이킹 레시피.'}]}"]},"metadata":{},"execution_count":65}]},{"cell_type":"code","source":[],"metadata":{"id":"Tku2RzGqQNF7"},"execution_count":null,"outputs":[]}]}