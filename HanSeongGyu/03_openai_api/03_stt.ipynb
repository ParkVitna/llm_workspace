{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNMubfR9E4zHu/9Ll0Q9ncv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# STT speech to text\n"],"metadata":{"id":"Evr35oMe1Qie"}},{"cell_type":"markdown","source":["## Whisper\n","\n","https://platform.openai.com/docs/models/whisper-1\n","\n","Whisper는 OpenAI에서 개발한 범용 음성 인식 모델로, 다양한 오디오 데이터셋을 학습하여 다국어 음성 인식, 음성 번역, 언어 식별 등의 작업을 수행할 수 있다.\n","\n","Whisper v2-large 모델은 현재 API를 통해 'whisper-1'이라는 이름으로 제공되고 있다.\n","\n","오픈 소스 버전의 Whisper와 API를 통한 Whisper는 기능적으로 동일하지만, API를 통해 제공되는 버전은 최적화된 추론 과정을 거쳐 다른 방법에 비해 더 빠르게 동작한다."],"metadata":{"id":"jl3RJlKQ1YxO"}},{"cell_type":"code","execution_count":2,"metadata":{"id":"7I771OtU1OIL","executionInfo":{"status":"ok","timestamp":1750835100773,"user_tz":-540,"elapsed":478,"user":{"displayName":"seongkyu han","userId":"12944266547765922383"}}},"outputs":[],"source":["from google.colab import userdata\n","\n","OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')"]},{"cell_type":"code","source":["from openai import OpenAI\n","\n","client = OpenAI(api_key=OPENAI_API_KEY)\n","\n","file_path = 'output.mp3'\n","\n","with open(file_path, 'rb') as f:\n","    transcription = cilent.audio.transcriptions.create(\n","        model ='whisper-1',\n","        file=f\n","    )\n","\n","print(transcription)\n","print(transcription.text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"J76zFPzf5rMs","executionInfo":{"status":"error","timestamp":1750835307069,"user_tz":-540,"elapsed":206,"user":{"displayName":"seongkyu han","userId":"12944266547765922383"}},"outputId":"c040e5a9-633d-488d-e485-f9acc47d1342"},"execution_count":3,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'output.mp3'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3-2296684892.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfile_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'output.mp3'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     transcription = cilent.audio.transcriptions.create(\n\u001b[1;32m      9\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;34m'whisper-1'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output.mp3'"]}]},{"cell_type":"code","source":[],"metadata":{"id":"n7WKHmvR54Eu"},"execution_count":null,"outputs":[]}]}