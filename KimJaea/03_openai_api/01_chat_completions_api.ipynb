{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPnUXbwaVc6wYkiXRItANkj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Chat Completions API\n","\n","# Chat Completions API\n","\n","Chat Completions API는 OpenAI에서 제공하는 대화형 인공지능 모델(GPT 계열)을 활용해, 사용자의 메시지에 대해 자연스러운 대화 응답을 생성하는 API이다. 이 API는 챗봇, AI 비서, 자동화된 상담 시스템 등 다양한 대화형 서비스에 적용할 수 있다.\n","\n","\n","- **대화 문맥 유지**  \n","  Chat Completions API는 여러 메시지(대화 내역)를 입력받아, 이전 대화의 맥락을 이해하고 그에 맞는 응답을 생성한다. 즉, 단순히 한 문장만을 이어 쓰는 것이 아니라, 대화의 흐름을 반영하여 자연스러운 대화를 이어갈 수 있다.\n","\n","- **역할(Role) 기반 메시지 구조**  \n","  입력 메시지는 배열 형태로 전달하며, 각 메시지는 `role`과 `content`로 구성된다.  \n","  - `system`: AI의 태도, 성격, 역할을 정의(예: \"너는 친절한 도우미야.\")\n","  - `user`: 사용자의 질문이나 요청\n","  - `assistant`: AI의 응답(이전 대화 내용 포함 가능)\n","  \n","  이 구조를 통해 AI의 응답 스타일이나 맥락을 세밀하게 제어할 수 있다[1][3][5].\n","\n","**주요 파라미터 설명**\n","\n","| 파라미터        | 설명                                                                 |\n","|----------------|----------------------------------------------------------------------|\n","| model          | 사용할 언어 모델명 (예: gpt-3.5-turbo, gpt-4o 등)                    |\n","| messages       | 대화 내역(역할/내용 포함) 배열                                        |\n","| max_tokens     | 생성할 응답의 최대 토큰 수(선택)                                     |\n","| temperature    | 창의성 조절(0~2, 낮을수록 일관성↑, 높을수록 다양성↑, 선택)           |\n","| top_p          | 누적 확률 기반 샘플링(temperature와 유사, 선택)                      |\n","| n              | 한 번에 생성할 응답 개수(선택)                                       |\n","| stop           | 응답 생성을 중단할 문자열 목록(선택)                                 |\n","| presence_penalty, frequency_penalty | 반복 억제 및 창의성 유도(선택)                 |\n","| user           | 사용자 식별자(선택, abuse monitoring 등 활용)                        |\n","\n","\n","- 위 예시에서 `messages` 배열에는 대화의 모든 메시지가 순서대로 들어가야 한다.  \n","- OpenAI는 이전 요청을 기억하지 않기 때문에, 매 API 호출마다 대화 내역 전체를 함께 보내야 한다."],"metadata":{"id":"iyBvgWsR7w9J"}},{"cell_type":"code","execution_count":3,"metadata":{"id":"v6gj9Fzw7tFV","executionInfo":{"status":"ok","timestamp":1750819304370,"user_tz":-540,"elapsed":469,"user":{"displayName":"김재아","userId":"11416159014926875591"}}},"outputs":[],"source":["from google.colab import userdata\n","import os\n","from openai import OpenAI\n","\n","# OPENAI_API_KEY = userdata.get(\"OPENAI_API_KEY\")\n","# client = OpenAI(api_key = OPENAI_API_KEY)\n","\n","# 환경변수 지정\n","os.environ[\"OPENAI_API_KEY\"] = userdata.get(\"OPENAI_API_KEY\")\n","client = OpenAI()"]},{"cell_type":"markdown","source":["## 대화형 챗봇"],"metadata":{"id":"JQrMjMa49ohZ"}},{"cell_type":"code","source":["response = client.chat.completions.create(\n","    model='gpt-4o-mini',\n","    messages=[\n","        {'role': 'system', 'content': '너는 친절한 챗봇입니다.'},\n","        {'role': 'user', 'content': '안녕, 내 이름은 차은우야~'},\n","        {'role': 'assistant', 'content': '안녕하세요, 차은우님! 만나서 반가워요. 어떻게 도와드릴까요?'},\n","        {'role': 'user', 'content': '잘 지냈어? 내 이름 기억하니?'},\n","    ]\n",")\n","print(response.choices[0].message.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L54I4pfl81jB","executionInfo":{"status":"ok","timestamp":1750822358662,"user_tz":-540,"elapsed":1470,"user":{"displayName":"김재아","userId":"11416159014926875591"}},"outputId":"47f40ed3-dfb9-4fb0-d0ab-daf283f7cc8b"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["네, 차은우님! 잘 지내셨어요? 궁금한 것이나 이야기하고 싶은 것이 있으면 말씀해 주세요!\n"]}]},{"cell_type":"code","source":["response = client.chat.completions.create(\n","    model='gpt-4o-mini',\n","    messages=[\n","        {'role': 'system', 'content': '너는 LLM 전문가입니다.'},\n","        {'role': 'user', 'content': '안녕, 나는 LLM 꿈나무 차은우야~'},\n","        {'role': 'assistant', 'content': '안녕하세요, 차은우님! LLM 꿈나무라니 멋지네요! 어떤 부분에 대해 이야기하고 싶으신가요? LLM에 대한 질문이나 궁금한 점이 있다면 언제든지 말씀해 주세요.'},\n","        {'role': 'user', 'content': 'Transformer모델을 공부하고 싶어.'},\n","        {'role': 'assistant', 'content': \"\"\"좋아요! Transformer 모델은 자연어 처리(NLP) 분야에서 매우 중요한 혁신을 가져온 아키텍처입니다. 기본적인 구조와 작동 원리를 설명해드릴게요.\n","\n","### 1. Transformer의 기본 구조\n","Transformer는 크게 두 가지 부분으로 구성되어 있습니다: **인코더(Encoder)**와 **디코더(Decoder)**.\n","\n","- **인코더**: 입력 문장을 처리하고, 문장의 의미를 포착하는 고차원 표현을 생성합니다.\n","- **디코더**: 인코더의 출력을 바탕으로 출력 문장을 생성합니다. 주로 번역, 텍스트 요약 등에 사용됩니다.\n","\n","### 2. 자기 주의 메커니즘 (Self-Attention)\n","Transformer의 핵심 개념 중 하나는 자기 주의 메커니즘입니다. 이는 입력 시퀀스 내의 단어들이 서로에게 얼마나 주의를 기울이는지를 학습하는 방법입니다. 이를 통해 문맥을 더 잘 이해할 수 있습니다.\n","\n","### 3. 포지셔널 인코딩 (Positional Encoding)\n","Transformers는 순차적인 정보를 처리하지 않기 때문에 단어의 순서를 이해하기 위한 포지셔널 인코딩을 사용합니다. 이를 통해 단어의 위치 정보를 추가합니다.\n","\n","### 4. 다중 헤드 주의 (Multi-head Attention)\n","Transformer는 하나의 자기 주의 메커니즘 대신 여러 개의 주의 헤드를 사용하여 서로 다른 표현을 학습합니다. 각 헤드는 다른 부분에 초점을 맞출 수 있어 정보의 다양성을 높입니다.\n","\n","### 5. 피드포워드 신경망\n","각 인코더와 디코더의 구조에는 자기 주의 레이어 뒤에 위치한 피드포워드 신경망이 있습니다. 이는 각 단어의 표현을 독립적으로 처리하는 역할을 합니다.\n","\n","### 6. Layer Normalization & Residual Connections\n","모든 레이어에는 레이어 정규화와 잔차 연결이 포함되어 있어, 훈련이 더 쉽게 이루어지도록 돕습니다.\n","\n","### 학습 방법\n","Transformer 모델은 대량의 데이터를 통해 사전 학습(pre-training)되고, 특정 작업에 대해 미세 조정(fine-tuning)됩니다. 여러 가지 변종이 있으며, BERT, GPT 같은 모델들이 이 구조를 기반으로 하고 있습니다.\n","\n","이해가 잘 되셨나요? 더 궁금한 부분이나 구체적인 질문이 있다면 말씀해 주세요!\n","\"\"\"},\n","        {'role': 'user', 'content': '어려워~ 어텐션을 초등학생도 이해할 수 있게 설명해줘.'},\n","        {'role': 'assistant', 'content': \"\"\"물론이죠! 어텐션(attention)을 초등학생도 이해할 수 있게 쉽게 설명해볼게요.\n","\n","### 어텐션을 설명하는 이야기\n","\n","상상해보세요! 여러분이 친구들과 함께 학교에서 이야기를 나누고 있다고 합시다. 그 많은 친구들 중에서 누군가가 아주 중요한 이야기를 하고 있어요. 여러분은 그 친구의 말을 잘 듣고 싶어서 그 친구에게 집중합니다. 반면, 주변의 다른 소리나 대화는 잘 듣지 않으려고 해요. 이런 식으로 우리는 중요한 정보에 주의를 기울이고, 다른 것들은 잠깐 잊어버리는 거예요.\n","\n","### 어텐션은 같아요!\n","\n","어텐션은 컴퓨터가 텍스트를 읽을 때 어떻게 중요한 단어에 집중하는지를 설명하는 방법이에요. 예를 들어, \"내가 어제 강아지와 함께 놀았어요\"라는 문장이 있다고 해볼까요. 이 문장에서 \"강아지\"라는 단어는 이야기에 중요한 부분이에요. 어텐션 메커니즘은 이 \"강아지\"라는 단어에 더 많은 주의를 기울이고, 나머지 단어는 그만큼 덜 신경 쓰는 거죠.\n","\n","### 간단한 예시\n","\n","이제 조금 더 구체적인 예를 들어볼게요. 만약 여러분이 \"사과는 빨갛고 맛있다\"라는 문장을 읽고 있을 때, 어텐션은 \"사과\"라는 단어에 특히 주목하게 해요. 왜냐하면 이 단어가 문장에서 가장 중요한 의미를 갖고 있기 때문이죠.\n","\n","어텐션은 이처럼 중요한 정보를 찾아내어 컴퓨터가 더 똑똑하게 텍스트를 이해하도록 도와주는 역할을 해요.\n","\n","이해가 안 되거나 더 알고 싶은 부분이 있다면 언제든지 물어보세요!\n","\"\"\"},\n","        {'role':'user', 'content':'이해가 되는 것같아. 트랜스포머 내용을 요약해서 하나의 markdown문서로 정리해줘'}\n","    ]\n",")\n","print(response.choices[0].message.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KBr8uD-k-aeI","executionInfo":{"status":"ok","timestamp":1750822504510,"user_tz":-540,"elapsed":9197,"user":{"displayName":"김재아","userId":"11416159014926875591"}},"outputId":"bd2f3dd0-e044-4e5a-a441-93dd38f6c444"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["물론입니다! 트랜스포머에 대한 내용을 요약하여 Markdown 문서 형태로 정리해보겠습니다.\n","\n","```markdown\n","# Transformer 모델 개요\n","\n","Transformer 모델은 자연어 처리(NLP) 분야에서 매우 중요한 혁신을 이룬 아키텍처입니다. 다음은 Transformer의 주요 구성 요소와 개념에 대한 간단한 설명입니다.\n","\n","## 1. 기본 구조\n","Transformer는 두 가지 주요 부분으로 나뉩니다: **인코더(Encoder)**와 **디코더(Decoder)**.\n","\n","- **인코더**: 입력 문장을 처리하고, 문장의 의미를 포착하는 고차원 표현을 생성합니다.\n","- **디코더**: 인코더의 출력을 바탕으로 출력 문장을 생성합니다.\n","\n","## 2. 자기 주의 메커니즘 (Self-Attention)\n","Transformer의 핵심 개념으로, 입력 시퀀스 내의 단어들이 서로에게 얼마나 주의를 기울이는지를 학습합니다. 이를 통해 문맥을 더 잘 이해할 수 있습니다.\n","\n","## 3. 포지셔널 인코딩 (Positional Encoding)\n","Transformer는 순차적인 정보를 처리하지 않기 때문에 단어의 순서를 이해하기 위해 포지셔널 인코딩을 사용하여 위치 정보를 추가합니다.\n","\n","## 4. 다중 헤드 주의 (Multi-head Attention)\n","여러 개의 주의 헤드를 사용하여 서로 다른 표현을 학습합니다. 각 헤드는 정보의 다양한 부분에 초점을 맞출 수 있습니다.\n","\n","## 5. 피드포워드 신경망\n","각 인코더와 디코더는 자기 주의 레이어 뒤에 피드포워드 신경망을 가지고 있어, 각 단어의 표현을 독립적으로 처리합니다.\n","\n","## 6. 레이어 정규화 및 잔차 연결\n","모든 레이어에는 레이어 정규화와 잔차 연결이 포함되어 있어 훈련이 더 쉽고 안정적으로 이루어지도록 돕습니다.\n","\n","## 학습 방법\n","Transformer 모델은 대량의 데이터를 통해 사전 학습(pre-training)되고, 특정 작업에 대해 미세 조정(fine-tuning)됩니다.\n","\n","## 결론\n","Transformer는 강력한 자연어 처리 모델로, BERT, GPT와 같은 다양한 변종이 이 아키텍처를 기반으로 하고 있습니다.\n","```\n","\n","이 문서를 통해 Transformer 모델의 기본 구조와 개념을 간단히 정리할 수 있습니다. 필요한 추가 사항이나 질문이 있다면 언제든지 말씀해 주세요!\n"]}]},{"cell_type":"markdown","source":["## 반복처리"],"metadata":{"id":"CWMtx5eDLWj5"}},{"cell_type":"code","source":["# 대화내역을 로깅\n","messages = [\n","    {'role': 'system', 'content': '너는 친절한 챗봇이다.'}\n","]\n","\n","print('종료하려면, exit를 입력하세요...')\n","while True:\n","    # 사용자 입력\n","    user_input = input('User: ')\n","\n","    if user_input.strip().lower() == 'exit':\n","        print('채팅을 종료합니다...')\n","        break\n","\n","    # messages에 사용자 입력 추가\n","    messages.append({'role': 'user', 'content': user_input})\n","\n","    # LLM 요청\n","    response = client.chat.completions.create(\n","        model='gpt-4.1',\n","        messages=messages,\n","        temperature=1\n","    )\n","    assistant_message = response.choices[0].message.content\n","    print(f'Assistant: {assistant_message}')\n","\n","    # messages 챗봇 출력 추가\n","    messages.append({'role': 'assistant','content': assistant_message})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":408},"id":"YC6x8DAfJyPm","executionInfo":{"status":"error","timestamp":1750823262780,"user_tz":-540,"elapsed":8517,"user":{"displayName":"김재아","userId":"11416159014926875591"}},"outputId":"00a76503-a0fc-4fc2-d880-978384ac30ae"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stdout","text":["종료하려면, exit를 입력하세요...\n","User: 안녕\n","Assistant: 안녕하세요! 만나서 반가워요. 😊 무엇을 도와드릴까요?\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"Interrupted by user","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-10-40149298.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# 사용자 입력\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0muser_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'User: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0muser_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'exit'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"]}]},{"cell_type":"code","source":[],"metadata":{"id":"Hgn8GJLtMfiP"},"execution_count":null,"outputs":[]}]}