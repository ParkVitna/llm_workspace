{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPfdZOXPt5EP0PB7WTCeBoP"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Model IO\n","- Language Model\n","- Prompt\n","- OutputParser\n","\n","  ### Langchain  Components\n","\n","  <img src=\"https://d.pr/i/Wy5B5B+\" width=\"500\"/>"],"metadata":{"id":"TGgWWeU2fmhu"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"oyKFpmMdfhIu","executionInfo":{"status":"ok","timestamp":1751001868717,"user_tz":-540,"elapsed":9719,"user":{"displayName":"조성재","userId":"00790730790294410246"}},"outputId":"696d560f-b65b-4dd5-c72c-6f10249978ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n","Requirement already satisfied: langchain-openai in /usr/local/lib/python3.11/dist-packages (0.3.26)\n","Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.11/dist-packages (0.3.0)\n","Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.26)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.66)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n","Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.1)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.91.0)\n","Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n","Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.1)\n","Requirement already satisfied: huggingface-hub>=0.30.2 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.33.0)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n","Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n","Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n","Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.5.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (2025.3.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (24.2)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (4.67.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (4.14.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (1.1.5)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n","Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.6.15)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n"]}],"source":["!pip install langchain langchain-openai langchain-huggingface langchain-community"]},{"cell_type":"code","source":["from google.colab import userdata\n","import os\n","os.environ['LANGSMITH_TRACING'] = userdata.get('LANGSMITH_TRACING')\n","os.environ['LANGSMITH_ENDPOINT'] = userdata.get('LANGSMITH_ENDPOINT')\n","os.environ['LANGSMITH_API_KEY'] = userdata.get('LANGSMITH_API_KEY')\n","os.environ['LANGSMITH_PROJECT'] = userdata.get('LANGSMITH_PROJECT')\n","os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')"],"metadata":{"id":"Edh5FPp9gPFD","executionInfo":{"status":"ok","timestamp":1751001871175,"user_tz":-540,"elapsed":2444,"user":{"displayName":"조성재","userId":"00790730790294410246"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## Language Models\n","\n","https://python.langchain.com/api_reference/reference.html#integrations\n","\n","LangChain의 Integrations 섹션에서는 다양한 다운스트림 LLM 모델과의 연동을 지원하다.\n","\n","이 섹션에서는 OpenAI, Hugging Face, GPT-4 등의 다양한 LLM 모델과 LangChain을 연결하는 방법을 다룬다."],"metadata":{"id":"NIxk08xagh1A"}},{"cell_type":"markdown","source":["### OpenAI"],"metadata":{"id":"B83fTIRPxIEx"}},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","\n","llm = ChatOpenAI(model_name='gpt-4o')\n","\n","llm.invoke('태국의 수도는 어디인가요?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L1sHwSx-gZcc","executionInfo":{"status":"ok","timestamp":1751001874183,"user_tz":-540,"elapsed":3004,"user":{"displayName":"조성재","userId":"00790730790294410246"}},"outputId":"4ef0e684-e306-4a19-c8c8-952da2d211e4"},"execution_count":3,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='태국의 수도는 방콕입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 16, 'total_tokens': 26, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a288987b44', 'id': 'chatcmpl-Bmvrd326L2epil2CXvdLTdPlV0ghe', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--4ecebeee-74c0-449f-9fb5-b01a3d3b5de4-0', usage_metadata={'input_tokens': 16, 'output_tokens': 10, 'total_tokens': 26, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"]},"metadata":{},"execution_count":3}]},{"cell_type":"markdown","source":["### huggingface"],"metadata":{"id":"_EboykKEyUrf"}},{"cell_type":"code","source":["from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n","\n","llm = HuggingFaceEndpoint(\n","    repo_id = 'microsoft/Phi-3-mini-4k-instruct',\n","    task='text-generation'\n",")\n","\n","chat_model = ChatHuggingFace(\n","    llm = llm,\n","    verbose=True\n",")\n","\n","chat_model.invoke('프랑스의 수도는 어디인가요?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5kHUGH03xd5f","executionInfo":{"status":"ok","timestamp":1751001885226,"user_tz":-540,"elapsed":11046,"user":{"displayName":"조성재","userId":"00790730790294410246"}},"outputId":"fad12793-fcf8-478b-e3c6-838a81992a83"},"execution_count":4,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='프랑스의 수도는 크리스마스에 있습니다. 크리스마스는 1294년에 바라며 다크 왕 려몰에서 언명한 나라이름의 도시인물이 있었습니다. 중세에서 혼성시대의 바라멸을 통해 프랑스에서도 정신과 다양한 섬세요. 두세기부터 파티잔치를 위해 기당 수도 구조를 변화시켰습니다. 수도는 현재 서울에 위치하며, 하림와 연체에 따르면 현지서명이 있는 시도였습니다.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 279, 'prompt_tokens': 25, 'total_tokens': 304}, 'model_name': 'microsoft/Phi-3-mini-4k-instruct', 'system_fingerprint': '3.2.1-sha-4d28897', 'finish_reason': 'stop', 'logprobs': None}, id='run--863a1a3a-dc15-4908-8122-1af2400f9fc6-0', usage_metadata={'input_tokens': 25, 'output_tokens': 279, 'total_tokens': 304})"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["# from langchain_huggingface import HuggingFacePipeline\n","\n","# pipe = HuggingFacePipeline.from_model_id(\n","#     model_id='microsoft/Phi-3-mini-4k-instruct',\n","#     task='text-generation'\n","# )\n","\n","# pipe.invoke('What is LLM?')"],"metadata":{"id":"fAy0i9p1yyn-","executionInfo":{"status":"ok","timestamp":1751001885231,"user_tz":-540,"elapsed":7,"user":{"displayName":"조성재","userId":"00790730790294410246"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["### ModelLaboratory\n","- 여러 LLM을 동시에 비교할 수 있는 실험도구"],"metadata":{"id":"u4U-DBBK1WYm"}},{"cell_type":"code","source":["from langchain.model_laboratory import ModelLaboratory\n","\n","llms = [\n","    ChatOpenAI(model_name='gpt-3.5-turbo'),\n","    ChatOpenAI(model_name='gpt-4.1'),\n","]\n","\n","lab = ModelLaboratory.from_llms(llms)\n","lab.compare('파이썬의 장점이 무엇인가요?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vadkK6rTz8NM","executionInfo":{"status":"ok","timestamp":1751001894015,"user_tz":-540,"elapsed":8777,"user":{"displayName":"조성재","userId":"00790730790294410246"}},"outputId":"a6c46cdf-5270-4bcc-90c3-ae0e31db1cba"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1mInput:\u001b[0m\n","파이썬의 장점이 무엇인가요?\n","\n","client=<openai.resources.chat.completions.completions.Completions object at 0x7f286f4c4710> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7f286dd30750> root_client=<openai.OpenAI object at 0x7f286e388250> root_async_client=<openai.AsyncOpenAI object at 0x7f286f4c5610> model_kwargs={} openai_api_key=SecretStr('**********')\n","\u001b[36;1m\u001b[1;3m1. 쉬운 학습 곡선: 파이썬은 간결하고 읽기 쉬운 문법을 가지고 있어 학습하기 쉽습니다. 또한, 다양한 자료구조와 내장 함수를 제공하여 프로그래밍을 보다 쉽게 할 수 있습니다.\n","\n","2. 다양한 라이브러리와 프레임워크: 파이썬은 다양한 라이브러리와 프레임워크를 제공하여 데이터 분석, 인공지능, 웹 개발 등 다양한 분야에서 사용되고 있습니다.\n","\n","3. 크로스 플랫폼 지원: 파이썬은 Windows, macOS, Linux 등 다양한 운영 체제에서 동일하게 작동하기 때문에 여러 플랫폼에서 사용할 수 있습니다.\n","\n","4. 커뮤니티 및 생태계: 파이썬은 활발한 개발자 커뮤니티와 다양한 오픈 소스 프로젝트가 존재하여 문제 해결 시 도움을 받을 수 있습니다.\n","\n","5. 높은 생산성: 파이썬은 다른 언어에 비해 코드를 작성하는 시간이 짧고 빠르기 때문에 빠른 개발이 가능합니다.\u001b[0m\n","\n","client=<openai.resources.chat.completions.completions.Completions object at 0x7f286dd315d0> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7f286dd33050> root_client=<openai.OpenAI object at 0x7f286dd320d0> root_async_client=<openai.AsyncOpenAI object at 0x7f286dd32390> model_name='gpt-4.1' model_kwargs={} openai_api_key=SecretStr('**********')\n","\u001b[33;1m\u001b[1;3m파이썬(Python)의 주요 장점은 다음과 같습니다:\n","\n","1. **문법이 간단하고 읽기 쉬움**  \n","코드가 영어와 비슷하게 읽히기 때문에 초보자도 쉽게 배울 수 있습니다. 들여쓰기를 사용해 코드 구조가 명확하게 보입니다.\n","\n","2. **다양한 라이브러리와 프레임워크**  \n","데이터 분석(Numpy, Pandas), 인공지능(TensorFlow, PyTorch), 웹 개발(Django, Flask) 등 거의 모든 분야에 사용 가능한 라이브러리와 프레임워크가 풍부합니다.\n","\n","3. **높은 생산성**  \n","복잡한 코드를 간결하게 작성할 수 있기 때문에 개발 속도가 빠릅니다.\n","\n","4. **플랫폼 독립적**  \n","운영체제(윈도우, 맥, 리눅스 등)에 상관없이 동일한 파이썬 코드를 실행할 수 있습니다.\n","\n","5. **대형 커뮤니티 지원**  \n","문제가 생기면 Stack Overflow, 공식 문서 등에서 도움을 쉽게 받을 수 있습니다.\n","\n","6. **응용 범위가 넓음**  \n","웹, 데이터 분석, 인공지능, 사물인터넷, 게임 등 다양한 분야에서 활용됩니다.\n","\n","7. **오픈 소스**  \n","무료로 사용할 수 있고, 누구나 자유롭게 수정/재배포할 수 있습니다.\n","\n","이런 장점들 때문에 파이썬은 초보 프로그래머부터 전문가까지, 다양한 분야에서 많이 사용되고 있습니다.\u001b[0m\n","\n"]}]},{"cell_type":"markdown","source":["## Prompts\n","\n","https://python.langchain.com/api_reference/core/prompts.html#langchain-core-prompts\n","\n","`LangChain`의 API 문서에서 제공하는 **Prompts**에 대한 내용은 LangChain 프레임워크의 **핵심 구성 요소 중 하나**로, LLM(Large Language Model)과의 인터페이스를 설정하는 데 중요한 역할을 한다. Prompts는 LLM에 전달될 입력을 정의하고, 구조화하며, 이를 기반으로 원하는 응답을 얻기 위해 사용된다.\n","\n","**주요 사용처**\n","\n","1. **자동화된 입력 구성**\n","   - PromptTemplate을 사용하여 사용자 입력을 자동으로 구성.\n","   - 동일한 형식의 질문이나 대화를 대량으로 생성 가능.\n","\n","2. **대화형 응답**\n","   - ChatPromptTemplate을 통해 대화형 AI의 문맥 유지를 지원.\n","\n","3. **샘플 기반 학습**\n","   - Few-shot Prompt는 LLM에 구체적인 예제를 제공해 정확한 응답을 유도.\n","\n","4. **결과 파싱**\n","   - Output Parsers를 통해 LLM의 출력을 특정 포맷으로 처리하여 후속 작업을 자동화.\n","\n","\n","**클래스 계층구조**\n","```\n","BasePromptTemplate\n","├─ PipelinePromptTemplate\n","├─ StringPromptTemplate\n","│  ├─ PromptTemplate\n","│  ├─ FewShotPromptTemplate\n","│  └─ FewShotPromptWithTemplates\n","└─ BaseChatPromptTemplate\n","   ├─ AutoGPTPrompt\n","   └─ ChatPromptTemplate\n","      └─ AgentScratchPadChatPromptTemplate\n","\n","BaseMessagePromptTemplate\n","├─ MessagesPlaceholder\n","└─ BaseStringMessagePromptTemplate\n","   ├─ ChatMessagePromptTemplate\n","   ├─ HumanMessagePromptTemplate\n","   ├─ AIMessagePromptTemplate\n","   └─ SystemMessagePromptTemplate\n","\n","```"],"metadata":{"id":"FUMmn7Yb2gRi"}},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","\n","llm = ChatOpenAI(model_name='gpt-4o-mini')\n","\n","llm.invoke(\"LLM이 뭐에요\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-G6os0vO2mSD","executionInfo":{"status":"ok","timestamp":1751002282295,"user_tz":-540,"elapsed":3497,"user":{"displayName":"조성재","userId":"00790730790294410246"}},"outputId":"7ca75a9b-523d-4f25-83ce-4b36f28efd83"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='LLM은 \"Large Language Model\"의 약자로, 대규모 언어 모델을 의미합니다. 이러한 모델은 자연어 처리를 위해 설계된 인공지능 시스템으로, 많은 양의 텍스트 데이터를 기반으로 학습하여 언어의 구조, 의미, 패턴 등을 이해하고 생성할 수 있습니다. LLM은 질문에 답하거나, 글을 작성하고, 번역을 하거나, 대화를 나누는 등의 여러 가지 작업에 활용됩니다. 대표적인 예로는 OpenAI의 GPT 시리즈, Google\\'s BERT, 그리고 Facebook의 BART 등이 있습니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 124, 'prompt_tokens': 13, 'total_tokens': 137, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BmvyBQ7vHHaLxOHI6xEXQns11rpVu', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--eea2c9a6-b03e-4d5e-91f2-cc41d97baf9a-0', usage_metadata={'input_tokens': 13, 'output_tokens': 124, 'total_tokens': 137, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["messages = [\n","    ('system', '당신은 친절한 초등학생 전용 챗봇입니다. 초딩의 눈높이에 맞게 설명해주세요'),\n","    ('human', '랭체인이 뭔가요?')\n","]\n","llm.invoke(messages)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UIQT8Nvd3mpj","executionInfo":{"status":"ok","timestamp":1751002386397,"user_tz":-540,"elapsed":3486,"user":{"displayName":"조성재","userId":"00790730790294410246"}},"outputId":"9a3b0071-8ff8-4a26-d3cc-ed2097e83755"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='랭체인(RankChain)은 요즘 사람들이 웹에서 정보를 찾는 방법을 더 똑똑하게 만들어 주는 기술이에요. 쉽게 말해서, 여러 가지 정보를 잘 연결해서 필요한 답을 빨리 찾을 수 있도록 도와주는 거예요. 마치 친구와 함께 문제를 해결하는 것처럼요!\\n\\n그래서 랭체인은 컴퓨터가 정보를 더 잘 이해하고, 우리가 원하는 대답을 더 빠르게 찾아주는 데 도움을 주는 거랍니다. 만약 궁금한 게 있다면, 랭체인을 통해 더 정확한 정보를 쉽게 찾을 수 있을 거예요!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 130, 'prompt_tokens': 47, 'total_tokens': 177, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BmvzrLcJoOnG95TQKat7zuVaIEaGx', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--c76c4e57-80fe-45bb-b9d2-37cf4e0891d8-0', usage_metadata={'input_tokens': 47, 'output_tokens': 130, 'total_tokens': 177, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["### PromptTemplete"],"metadata":{"id":"GEaJENx44Ox-"}},{"cell_type":"code","source":["from langchain import PromptTemplate\n","\n","# 어떤 상품에 대한 광고문구를 생성\n","\n","prompt_template = PromptTemplate(\n","    template='{product}를 홍보하기 위한 신박한 광고문구를 작성해줘',\n","    input_variables=['product']\n",")\n","prompt = prompt_template.format(product='초소형 카메라')\n","prompt = prompt_template.format(product='냉털용 냉장고')\n","\n","ai_message = llm.invoke(prompt)\n","print(ai_message.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"demNJI0D4AE3","executionInfo":{"status":"ok","timestamp":1751002936744,"user_tz":-540,"elapsed":3697,"user":{"displayName":"조성재","userId":"00790730790294410246"}},"outputId":"1d81d2d4-d278-4c1e-f8f4-bf388e7396cd"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["\"당신의 냉장고도 스타일로 가득 채우세요! 💫\n","\n","🌟 냉털용 냉장고, 이제는 단순한 저장공간이 아닙니다!  \n","신선함을 지키며, 당신의 개성을 담아주는 멋진 동반자가 되어드립니다.  \n","\n","✅ 공간 활용의 정수: 컴팩트한 디자인으로 어디에든 잘 어울리며, 세련된 인테리어를 완성해드립니다.  \n","✅ 모든 식품의 신선함을 지켜주는 최첨단 냉각 기술. 건강한 라이프스타일의 시작!  \n","✅ eco-friendly 옵션으로 지구를 생각하는 당신을 위한 선택!  \n","\n","냉털용 냉장고와 함께라면, 신선함과 스타일이 한 자리에! 지금 바로 만나보세요!\"\n"]}]},{"cell_type":"markdown","source":["### ChatPromptTemplate"],"metadata":{"id":"5pXMVoLA5zrl"}},{"cell_type":"code","source":["from langchain.prompts.chat import (\n","    ChatPromptTemplate,\n","    SystemMessagePromptTemplate,\n","    HumanMessagePromptTemplate\n",")\n","\n","system_msg_template = SystemMessagePromptTemplate.from_template(\"당신은 {domain}분야의 최고의 챗봇입니다.\")\n","human_msg_template = HumanMessagePromptTemplate.from_template(\"{question}\")\n","chat_template = ChatPromptTemplate.from_messages([\n","    system_msg_template, human_msg_template\n","])\n","prompt = chat_template.format_messages(domain='IT', question='SLM이 뭐야')\n","print(llm.invoke(prompt).content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jcJPLDlX5Gvy","executionInfo":{"status":"ok","timestamp":1751003320639,"user_tz":-540,"elapsed":4644,"user":{"displayName":"조성재","userId":"00790730790294410246"}},"outputId":"1d9872fe-7497-447d-85e8-a6d1f4f21fa3"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["SLM은 \"Service Level Management\"의 약자로, 서비스 수준 관리를 의미합니다. SLM은 IT 서비스 관리(ITSM)와 관련된 개념으로, 조직이 제공하는 서비스의 품질을 정의하고 유지하기 위한 프로세스와 활동을 포함합니다. 주요 목적은 서비스 제공자와 고객 간의 기대치를 조정하고, 서비스 성과를 측정하며, 이를 지속적으로 개선하는 것입니다.\n","\n","SLM의 주요 구성 요소는 다음과 같습니다:\n","\n","1. **서비스 수준 목표(SLAs)**: 서비스 제공에 대한 성과 기준을 명시하는 문서로, 특정 서비스가 충족해야 하는 요구 사항과 기대치를 정의합니다.\n","\n","2. **서비스 수준 보고서**: 서비스 성과를 정기적으로 검토하고 보고하여 고객과의 투명성을 유지합니다.\n","\n","3. **서비스 개선 계획**: 고객의 피드백과 성과 데이터를 기반으로, 서비스 수준을 향상시키기 위한 전략과 조치를 마련합니다.\n","\n","SLM은 고객 만족도를 높이고, 서비스 품질을 유지하며, 비즈니스 목표를 지원하기 위해 필수적인 요소입니다.\n"]}]},{"cell_type":"code","source":["prompt = chat_template.format_messages(domain='육아', question='우리 아이가 계속 울어요')\n","print(llm.invoke(prompt).content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yXUc6nL27FjG","executionInfo":{"status":"ok","timestamp":1751003306923,"user_tz":-540,"elapsed":4670,"user":{"displayName":"조성재","userId":"00790730790294410246"}},"outputId":"5920d057-702c-4feb-cb2e-ddbbf94aa882"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["아이가 계속 우는 것은 부모에게 힘든 상황일 수 있습니다. 아이가 우는 이유는 다양할 수 있으니 몇 가지 점을 확인해 보세요.\n","\n","1. **배고픔**: 아이가 배고픈 경우가 많습니다. 마지막으로 먹은 시간이 얼마나 되었는지 확인해 보세요.\n","2. **기저귀**: 기저귀가 더러워져서 불편할 수 있습니다. 기저귀 상태를 체크해 보세요.\n","3. **피로**: 아이가 너무 피곤해할 수 있습니다. 적절한 수면 시간을 갖고 있는지 확인해 보세요.\n","4. **불편함**: 옷이 너무 꽉 차거나, 너무 덥거나 추운 환경에 있을 수도 있습니다.\n","5. **감정적 이유**: 아이가 불안하거나, 안아달라는 기분일 수도 있습니다. 달래주거나 안아주는 것이 도움이 될 수 있습니다.\n","\n","이 외에도 아이의 특별한 요구나 건강 문제 등도 있을 수 있으니 상황에 따라 적절히 대응해 주세요. 필요하다면 전문가와 상담하는 것도 좋은 방법입니다.\n"]}]},{"cell_type":"markdown","source":["### FewShotPromptTemplate"],"metadata":{"id":"XxF3UAL_ClfH"}},{"cell_type":"code","source":["from langchain.prompts import FewShotPromptTemplate\n","\n","exmaples = [\n","    {'q': '2 + 2 = ?', 'a': '4'},\n","    {'q': '3 + 5 = ?', 'a': '8'}\n","]\n","\n","prompt_template = PromptTemplate(\n","    template='Q: {q}\\nA: {a}',\n","    input_variables=['q', 'a']\n",")\n","\n","fewshot_template = FewShotPromptTemplate(\n","    examples=exmaples,\n","    example_prompt=prompt_template,\n","    prefix='다음 수학문제를 풀어주세요(답변은 정답만 출력하세요):',\n","    suffix='Q: {question} \\nA:', # 사용자 입력값\n","    input_variables=['question']\n",")\n","\n","prompt = fewshot_template.format(question='12367 + 345 = ?')\n","print(prompt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X-PVuYZu7ghW","executionInfo":{"status":"ok","timestamp":1751005669425,"user_tz":-540,"elapsed":9,"user":{"displayName":"조성재","userId":"00790730790294410246"}},"outputId":"bd8691ca-c025-480f-9ca5-c5789015587f"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["다음 수학문제를 풀어주세요(답변은 정답만 출력하세요):\n","\n","Q: 2 + 2 = ?\n","A: 4\n","\n","Q: 3 + 5 = ?\n","A: 8\n","\n","Q: 12367 + 345 = ? \n","A:\n"]}]},{"cell_type":"code","source":["print(llm.invoke(prompt).content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YzYtLDSID45K","executionInfo":{"status":"ok","timestamp":1751005670676,"user_tz":-540,"elapsed":577,"user":{"displayName":"조성재","userId":"00790730790294410246"}},"outputId":"3c45c00c-18c4-4356-d27a-1d5e63d9f902"},"execution_count":23,"outputs":[{"output_type":"stream","name":"stdout","text":["12712\n"]}]},{"cell_type":"markdown","source":["## Output Parsers\n","\n","https://python.langchain.com/api_reference/langchain/output_parsers.html#module-langchain.output_parsers\n","\n","LangChain의 Output Parsers는 LLM이 생성한 텍스트 출력을 특정 형식으로 변환하거나 처리하는 데 사용된다. 이는 모델의 응답을 해석하고, 이를 구조화된 데이터로 바꿔 후속 작업에 활용하기 위해 설계되었다. Output Parsers는 LangChain의 응답 처리 워크플로우에서 중요한 역할을 한다.\n","\n","예를 들어, LLM 응답이 \"Name: John, Age: 30\"와 같은 텍스트라면, 이를 {\"name\": \"John\", \"age\": 30}과 같은 Python 딕셔너리로 변환 가능.\n","\n","**사용 목적**\n","- 모델의 출력을 특정 애플리케이션에 맞게 처리해야 하는 경우가 많음.\n","- 응답을 해석하는 일관성과 정확성을 높이기 위해 필요.\n","- 텍스트 기반 응답을 JSON, 리스트 또는 숫자와 같은 특정 포맷으로 변환하여 후속 작업에 활용.\n","\n","**종류**\n","1. **BaseOutputParser**: Output Parsers의 기본 클래스, 커스텀 파서 구현 시 사용.  \n","2. **CommaSeparatedListOutputParser**: 콤마로 구분된 문자열을 리스트로 변환.  \n","3. **RegexParser**: 정규식을 사용해 특정 패턴을 추출하고 키-값 형태로 반환.  \n","4. **StructuredOutputParser**: 출력의 JSON 또는 구조화된 형식을 강제.  \n","5. **PydanticOutputParser**: Pydantic 모델을 기반으로 출력 검증 및 변환.  \n","6. **MarkdownOutputParser**: 마크다운 형식의 텍스트에서 데이터를 추출.  "],"metadata":{"id":"5SfzZBw5EtmA"}},{"cell_type":"markdown","source":["### CommaSeparatedListOutputParser"],"metadata":{"id":"7jddamJOFEFE"}},{"cell_type":"code","source":["from langchain.output_parsers import CommaSeparatedListOutputParser\n","\n","model_output = \"사과, 바나나, 오렌지, 포도\"\n","\n","output_parser = CommaSeparatedListOutputParser()\n","output = output_parser.parse(model_output)\n","output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rWW28riuEO0g","executionInfo":{"status":"ok","timestamp":1751005922072,"user_tz":-540,"elapsed":9,"user":{"displayName":"조성재","userId":"00790730790294410246"}},"outputId":"2d100cc3-fe3c-4a2d-ddac-7e14dc3cc572"},"execution_count":24,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['사과', '바나나', '오렌지', '포도']"]},"metadata":{},"execution_count":24}]},{"cell_type":"code","source":["# {야구팀} {5}개 질문\n","# {축구팀} {10}개 질문\n","\n","prompt_template = PromptTemplate(\n","    template=\"{subject} {n}개의 팀을 보여주세요.\\n{format_instruction}\",\n","    input_variables=['subject', 'n'], # 사용자 프롬프트로 채워질 변수\n","    partial_variables={\n","        # template 생성시에 채워짐\n","        'format_instruction': output_parser.get_format_instructions()\n","    }\n",")\n","prompt = prompt_template.format(subject='대한민국 프로야구', n=5) # format 함수때문에 template은 f를 중첩으로 쓰면 안됨\n","prompt = prompt_template.format(subject='프리미어리그', n=5)\n","prompt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"AWOAKlAvFgH0","executionInfo":{"status":"ok","timestamp":1751006426539,"user_tz":-540,"elapsed":12,"user":{"displayName":"조성재","userId":"00790730790294410246"}},"outputId":"871da0ac-fb48-413a-dd7a-e15e77d3c161"},"execution_count":27,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'프리미어리그 5개의 팀을 보여주세요.\\nYour response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":27}]},{"cell_type":"code","source":["ai_message = llm.invoke(prompt)\n","output = ai_message.content\n","\n","# 출력 파서가 가공한 최종출력\n","output = output_parser.parse(output)\n","output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dTB4ytlJG0h5","executionInfo":{"status":"ok","timestamp":1751006529111,"user_tz":-540,"elapsed":995,"user":{"displayName":"조성재","userId":"00790730790294410246"}},"outputId":"ebdcb54c-05d5-4807-b013-4967414e4b92"},"execution_count":29,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['맨체스터 유나이티드', '리버풀', '첼시', '맨체스터 시티', '아스날']"]},"metadata":{},"execution_count":29}]},{"cell_type":"code","source":["chain = prompt_template | llm | output_parser\n","chain.invoke(input={'subject': '프로농구', 'n':3})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"i4oc5lSmHonU","executionInfo":{"status":"ok","timestamp":1751006827265,"user_tz":-540,"elapsed":675,"user":{"displayName":"조성재","userId":"00790730790294410246"}},"outputId":"f1cf2fa0-6aaf-4224-cafb-5d45d52dfa3a"},"execution_count":32,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['서울 삼성', '전주 KCC', '인천 전자랜드']"]},"metadata":{},"execution_count":32}]},{"cell_type":"markdown","source":["### JSONOutputParser"],"metadata":{"id":"H7H_t6aJJEWg"}},{"cell_type":"code","source":["from langchain_core.output_parsers import JsonOutputParser\n","\n","model_output = '{\"title\": \"GPT-5를 소개합니다.\", \"author\": \"OpenAI\", \"pages\": 250}'\n","\n","json_parser = JsonOutputParser()\n","print(json_parser.get_format_instructions())\n","\n","output = json_parser.parse(model_output) # json_str -> python object(list, dict)\n","print(output)\n","print(type(output))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-jZyF-7uIW-E","executionInfo":{"status":"ok","timestamp":1751009046012,"user_tz":-540,"elapsed":28,"user":{"displayName":"조성재","userId":"00790730790294410246"}},"outputId":"084c84d0-42d1-4ffc-b53f-861748307a16"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Return a JSON object.\n","{'title': 'GPT-5를 소개합니다.', 'author': 'OpenAI', 'pages': '250'}\n","<class 'dict'>\n"]}]},{"cell_type":"code","source":["# AI 관련 책 3권을 보여주세요. (json)\n","json_parser = JsonOutputParser()\n","\n","prompt_template = PromptTemplate(\n","    template=\"{subject} 관련 한국어 {n}개의 책을 보여주세요.\\n{format_instruction}\",\n","    input_variables=['subject', 'n'], # 사용자 프롬프트로 채워질 변수\n","    partial_variables={\n","        # template 생성시에 채워짐\n","        'format_instruction': json_parser.get_format_instructions()\n","    }\n",")\n","\n","llm = ChatOpenAI(model_name='gpt-4o-mini')\n","\n","prompt = prompt_template.format(subject='AI', n=3) # format 함수때문에 template은 f를 중첩으로 쓰면 안됨\n","\n","ai_message = llm.invoke(prompt)\n","output = ai_message.content\n","\n","# 출력 파서가 가공한 최종출력\n","output = json_parser.parse(output)\n","print(output)\n","\n","# 요리 관련 책 5권을 보여주세요. (json)\n","# PromptTemplate - LLM - JsonOutputParser"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kzwRpUlsJtvt","executionInfo":{"status":"ok","timestamp":1751009533470,"user_tz":-540,"elapsed":3445,"user":{"displayName":"조성재","userId":"00790730790294410246"}},"outputId":"3268c8c3-95a9-42de-b6c1-8351b03f2e6e"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["{'books': [{'title': '인공지능의 기초', 'author': '김성훈', 'publisher': '한빛미디어', 'year': 2020}, {'title': 'AI, 인공지능: 생각의 기초', 'author': '홍길동', 'publisher': '미래엔', 'year': 2021}, {'title': '데이터 과학과 인공지능', 'author': '이재홍', 'publisher': '길벗', 'year': 2022}]}\n"]}]},{"cell_type":"code","source":["chain = prompt_template | llm | json_parser\n","ouput = chain.invoke(input={'subject':'AI', 'n':3})\n","print(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"22DDJJjIP715","executionInfo":{"status":"ok","timestamp":1751009536378,"user_tz":-540,"elapsed":2905,"user":{"displayName":"조성재","userId":"00790730790294410246"}},"outputId":"db2ce940-b515-4762-9125-176627edffde"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["{'books': [{'title': '인공지능의 기초', 'author': '김성훈', 'publisher': '한빛미디어', 'year': 2020}, {'title': 'AI, 인공지능: 생각의 기초', 'author': '홍길동', 'publisher': '미래엔', 'year': 2021}, {'title': '데이터 과학과 인공지능', 'author': '이재홍', 'publisher': '길벗', 'year': 2022}]}\n"]}]},{"cell_type":"code","source":["#복잡한 작업을 단순하게, 파이프라인처럼 연결해서 처리할 수 있다\n","from langchain.prompts import PromptTemplate\n","from langchain.chat_models import ChatOpenAI\n","from langchain.output_parsers import JsonOutputParser\n","\n","prompt_template = PromptTemplate.from_template(\"Tell me {n} facts about {subject}.\")\n","llm = ChatOpenAI()\n","json_parser = JsonOutputParser()\n","\n","chain = prompt_template | llm | json_parser\n","\n","output = chain.invoke({\"subject\": \"AI\", \"n\": 3})\n","print(output)"],"metadata":{"id":"2BgRbkBwTKNi"},"execution_count":null,"outputs":[]}]}