{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNSNrOIOFMy+aGDF10xlN9T"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"9b85e7a24e794ab7b2a7ec8e381dbfcb":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8608f396336d48528806c12ad661d841","IPY_MODEL_64d6a20f701c46dfbbcd0e3b21fc9f14","IPY_MODEL_b110f8b43867469791daeb4bc9584786"],"layout":"IPY_MODEL_6ec121146e9c469d85adebee6548def0"}},"8608f396336d48528806c12ad661d841":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e5dd37fc9498419da22848f590657d4a","placeholder":"​","style":"IPY_MODEL_8525777d48bc4470ac90ae23bafc6c11","value":"Loading checkpoint shards:  50%"}},"64d6a20f701c46dfbbcd0e3b21fc9f14":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"","description_tooltip":null,"layout":"IPY_MODEL_e0dd2ab7835c42c082332885f0899db2","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b0218f25412a4b2cbca51b044e798f45","value":1}},"b110f8b43867469791daeb4bc9584786":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_206f38b1e9424f4e919e309bfd22e493","placeholder":"​","style":"IPY_MODEL_8b9dc749fd7747b4b56ed1238f95b99c","value":" 1/2 [00:20&lt;00:20, 20.37s/it]"}},"6ec121146e9c469d85adebee6548def0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5dd37fc9498419da22848f590657d4a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8525777d48bc4470ac90ae23bafc6c11":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e0dd2ab7835c42c082332885f0899db2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b0218f25412a4b2cbca51b044e798f45":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"206f38b1e9424f4e919e309bfd22e493":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b9dc749fd7747b4b56ed1238f95b99c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Model IO\n","<img src=\"https://d.pr/i/Wy5B5B+\" width=\"500\"/>\n","\n","- Language Model\n","- Prompt\n","- OutputParser"],"metadata":{"id":"97z40kOlgCbA"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4tdDZsvgfhOn","executionInfo":{"status":"ok","timestamp":1751001862473,"user_tz":-540,"elapsed":10859,"user":{"displayName":"Jh K","userId":"12877179054230177413"}},"outputId":"8049ce71-b97c-4a8a-8641-5dacdb6f1b42"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain-openai in /usr/local/lib/python3.11/dist-packages (0.3.26)\n","Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.26)\n","Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.11/dist-packages (0.3.0)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.3.66)\n","Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.91.0)\n","Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n","Requirement already satisfied: langchain<1.0.0,>=0.3.26 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.3.26)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.41)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.32.3)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (6.0.2)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n","Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n","Requirement already satisfied: langsmith>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n","Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n","Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n","Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.1)\n","Requirement already satisfied: huggingface-hub>=0.30.2 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.33.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.5.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (2025.3.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (24.2)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (4.67.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (4.14.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (1.1.5)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (0.3.8)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain<1.0.0,>=0.3.26->langchain-community) (2.11.7)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain-openai) (1.33)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (3.10.18)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.125->langchain-community) (0.23.0)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n","Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (0.4.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain-community) (2025.6.15)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain-community) (3.2.3)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.125->langchain-community) (0.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain-openai) (3.0.0)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain<1.0.0,>=0.3.26->langchain-community) (2.33.2)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n"]}],"source":["!pip install langchain-openai langchain-community langchain-huggingface"]},{"cell_type":"code","source":["# colab secret 모두 등록할 것\n","from google.colab import userdata\n","import os\n","\n","# LANGSMITH_TRACING=true\n","# LANGSMITH_ENDPOINT=\"https://api.smith.langchain.com\"\n","# LANGSMITH_API_KEY=\"<your-api-key>\"\n","# LANGSMITH_PROJECT=\"skn14-langchain\"\n","# OPENAI_API_KEY=\"<your-openai-api-key>\"\n","\n","\n","os.environ['LANGSMITH_TRACING'] = userdata.get('LANGSMITH_TRACING')\n","os.environ['LANGSMITH_ENDPOINT'] = userdata.get('LANGSMITH_ENDPOINT')\n","os.environ['LANGSMITH_API_KEY'] = userdata.get('LANGSMITH_API_KEY')\n","os.environ['LANGSMITH_PROJECT'] = userdata.get('LANGSMITH_PROJECT')\n","os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n"],"metadata":{"id":"yOxZwrEQgJh9","executionInfo":{"status":"ok","timestamp":1751001864882,"user_tz":-540,"elapsed":2414,"user":{"displayName":"Jh K","userId":"12877179054230177413"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## Language Models\n","\n","https://python.langchain.com/api_reference/reference.html#integrations\n","\n","LangChain의 Integrations 섹션에서는 다양한 다운스트림 LLM 모델과의 연동을 지원하다.\n","\n","이 섹션에서는 OpenAI, Hugging Face, GPT-4 등의 다양한 LLM 모델과 LangChain을 연결하는 방법을 다룬다."],"metadata":{"id":"dpwmdqcMgb7B"}},{"cell_type":"markdown","source":["### openai"],"metadata":{"id":"ad8s39hfxWsT"}},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","\n","llm = ChatOpenAI(model_name='gpt-4o')\n","\n","llm.invoke('태국의 수도는 어디인가요?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oJ3jPg_PgY3T","executionInfo":{"status":"ok","timestamp":1751001904919,"user_tz":-540,"elapsed":2133,"user":{"displayName":"Jh K","userId":"12877179054230177413"}},"outputId":"efcf3388-59d0-47b7-889f-ffcc041ce56d"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='태국의 수도는 방콕입니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 10, 'prompt_tokens': 16, 'total_tokens': 26, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_07871e2ad8', 'id': 'chatcmpl-Bmvs8b20eY80dO3sb0MuSOIjza5kU', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--69e44ad9-b6f4-46e5-9dff-dcc47fb58151-0', usage_metadata={'input_tokens': 16, 'output_tokens': 10, 'total_tokens': 26, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["### huggingface"],"metadata":{"id":"gNxMr6fSyYf4"}},{"cell_type":"code","source":["from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n","\n","llm = HuggingFaceEndpoint(\n","    repo_id = 'microsoft/Phi-3-mini-4k-instruct',\n","    task='text-generation'\n",")\n","chat_model = ChatHuggingFace(\n","    llm=llm,\n","    verbose=True\n",")\n","\n","chat_model.invoke('서울은 어느 나라의 수도야?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GoPxoMHdyZaF","executionInfo":{"status":"ok","timestamp":1751001902787,"user_tz":-540,"elapsed":9440,"user":{"displayName":"Jh K","userId":"12877179054230177413"}},"outputId":"325f4d9e-3df6-4d2d-fa86-61730ec80681"},"execution_count":5,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='서울은 대한민국의 수도야. 대한민국는 북조선(조선)과 환경융합국가(한국과)와 같은 국가로 이루어져 있으며, 서울에서 \"삼질병(삼질병 연합회국)으로서\" 하나의 제도가 있기 때문에 경제적, 존재적이고 문화적으로도 중요한 지역이였으나, 실행이 되어 있는 국가가 아니었다. 현재는 서울는 대한민국의 수단중 초등지역.', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 247, 'prompt_tokens': 25, 'total_tokens': 272}, 'model_name': 'microsoft/Phi-3-mini-4k-instruct', 'system_fingerprint': '3.2.1-sha-4d28897', 'finish_reason': 'stop', 'logprobs': None}, id='run--dc4f630c-49a6-4ea1-8afb-5521f4d32c17-0', usage_metadata={'input_tokens': 25, 'output_tokens': 247, 'total_tokens': 272})"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["from langchain_huggingface import HuggingFacePipeline\n","\n","pipe = HuggingFacePipeline.from_model_id(\n","    model_id='microsoft/Phi-3-mini-4k-instruct',\n","    task='text-generation'\n",")\n","pipe.invoke('What is LLM?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["9b85e7a24e794ab7b2a7ec8e381dbfcb","8608f396336d48528806c12ad661d841","64d6a20f701c46dfbbcd0e3b21fc9f14","b110f8b43867469791daeb4bc9584786","6ec121146e9c469d85adebee6548def0","e5dd37fc9498419da22848f590657d4a","8525777d48bc4470ac90ae23bafc6c11","e0dd2ab7835c42c082332885f0899db2","b0218f25412a4b2cbca51b044e798f45","206f38b1e9424f4e919e309bfd22e493","8b9dc749fd7747b4b56ed1238f95b99c"]},"id":"a5WoVpx8ziqz","outputId":"7cec89f9-ca41-4e54-ff82-5359940fc65a"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b85e7a24e794ab7b2a7ec8e381dbfcb"}},"metadata":{}}]},{"cell_type":"markdown","source":["### ModelKaboratory\n","- 여러 LLM을 동시에 비교할 수 있는 실험도구"],"metadata":{"id":"2Bdra-v3zf2Y"}},{"cell_type":"code","source":["from langchain.model_laboratory import ModelLaboratory\n","\n","llms = [\n","    ChatOpenAI(model_name='gpt-3.5-turbo'),\n","    ChatOpenAI(model_name='gpt-4.1'),\n","]\n","\n","lab = ModelLaboratory.from_llms(llms)\n","lab.compare('파이썬의 장점이 무엇인가요?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EUfDWVr51aoT","executionInfo":{"status":"ok","timestamp":1751001918697,"user_tz":-540,"elapsed":10408,"user":{"displayName":"Jh K","userId":"12877179054230177413"}},"outputId":"f6d63220-f87d-431f-bc3a-333a2bd4cc2a"},"execution_count":7,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1mInput:\u001b[0m\n","파이썬의 장점이 무엇인가요?\n","\n","client=<openai.resources.chat.completions.completions.Completions object at 0x7b3c5aa60210> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7b3c58f974d0> root_client=<openai.OpenAI object at 0x7b3c5b5c5810> root_async_client=<openai.AsyncOpenAI object at 0x7b3c5acc3910> model_kwargs={} openai_api_key=SecretStr('**********')\n","\u001b[36;1m\u001b[1;3m1. 쉽고 간단한 문법: 파이썬은 다른 언어에 비해 문법이 간단하고 이해하기 쉽습니다. 이로 인해 초보자들도 빠르게 학습할 수 있습니다.\n","\n","2. 다양한 라이브러리와 모듈: 파이썬은 다양한 라이브러리와 모듈을 제공하여 개발자들이 빠르게 프로그램을 개발할 수 있습니다. \n","\n","3. 크로스 플랫폼 지원: 파이썬은 다양한 운영체제에서 사용할 수 있고, 이식성이 뛰어나기 때문에 여러 플랫폼에서 동작할 수 있습니다. \n","\n","4. 커뮤니티와 생태계: 파이썬은 커다란 개발자 커뮤니티와 다양한 오픈소스 라이브러리를 보유하고 있어, 지원과 자료가 풍부합니다.\n","\n","5. 데이터 분석 및 인공지능 분야에서 인기: 데이터 분석, 머신러닝, 딥러닝 등의 분야에서 파이썬이 인기를 끌고 있습니다. 데이터 과학 및 인공지능을 위한 라이브러리와 도구가 잘 갖춰져 있습니다. \n","\n","6. 확장성: 파이썬은 모듈과 패키지를 통해 기능을 확장하고 다양한 어플리케이션을 개발할 수 있습니다. \n","\n","7. 높은 생산성: 파이썬의 간결한 문법과 다양한 라이브러리들을 이용하여 개발 속도가 빠르고 생산성이 높습니다. \n","\n","8. 문서화와 테스팅: 파이썬은 문서화와 테스팅을 위한 다양한 도구들을 제공하여 소프트웨어 개발 과정을 보다 효율적으로 관리할 수 있습니다.\u001b[0m\n","\n","client=<openai.resources.chat.completions.completions.Completions object at 0x7b3c58fba710> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x7b3c58fb9850> root_client=<openai.OpenAI object at 0x7b3c58f87890> root_async_client=<openai.AsyncOpenAI object at 0x7b3c58fb96d0> model_name='gpt-4.1' model_kwargs={} openai_api_key=SecretStr('**********')\n","\u001b[33;1m\u001b[1;3m파이썬의 주요 장점은 다음과 같습니다:\n","\n","1. **쉬운 문법과 가독성**\n","   - 파이썬은 문법이 간단하고 코드가 읽기 쉬워 초보자도 쉽게 배울 수 있습니다.\n","   - 들여쓰기를 통해 코드 블록을 구분하며, 명확한 구조를 가집니다.\n","\n","2. **풍부한 라이브러리와 프레임워크**\n","   - 데이터 과학, 인공지능, 웹 개발, 자동화 등 다양한 분야에 특화된 방대한 라이브러리가 지원됩니다 (예: NumPy, pandas, TensorFlow, Django 등).\n","   - 개발 속도가 크게 빨라집니다.\n","\n","3. **다양한 플랫폼에서 사용 가능**\n","   - 운영 체제에 구애받지 않고(Windows, macOS, Linux 등) 사용할 수 있습니다.\n","\n","4. **높은 생산성**\n","   - 코드 작성이 간결하고 빠르게 개발할 수 있어 생산성이 높습니다.\n","\n","5. **인터프리터 언어**\n","   - 컴파일 과정 없이 바로 실행되므로 빠른 실험과 디버깅이 가능합니다.\n","\n","6. **강력한 커뮤니티와 자료**\n","   - 전 세계적으로 많은 사람들이 사용해 활발한 커뮤니티와 다양한 문서, 예제가 제공됩니다.\n","\n","7. **유연한 프로그래밍 패러다임**\n","   - 객체지향, 절차적, 함수형 프로그래밍 등 여러 패러다임을 지원합니다.\n","\n","요약하면, 파이썬은 배우기 쉽고 활용 범위가 넓으며 빠르게 개발할 수 있는 강력한 프로그래밍 언어입니다.\u001b[0m\n","\n"]}]},{"cell_type":"markdown","source":["## Prompts\n","\n","https://python.langchain.com/api_reference/core/prompts.html#langchain-core-prompts\n","\n","`LangChain`의 API 문서에서 제공하는 **Prompts**에 대한 내용은 LangChain 프레임워크의 **핵심 구성 요소 중 하나**로, LLM(Large Language Model)과의 인터페이스를 설정하는 데 중요한 역할을 한다. Prompts는 LLM에 전달될 입력을 정의하고, 구조화하며, 이를 기반으로 원하는 응답을 얻기 위해 사용된다.\n","\n","**주요 사용처**\n","\n","1. **자동화된 입력 구성**\n","   - PromptTemplate을 사용하여 사용자 입력을 자동으로 구성.\n","   - 동일한 형식의 질문이나 대화를 대량으로 생성 가능.\n","\n","2. **대화형 응답**\n","   - ChatPromptTemplate을 통해 대화형 AI의 문맥 유지를 지원.\n","\n","3. **샘플 기반 학습**\n","   - Few-shot Prompt는 LLM에 구체적인 예제를 제공해 정확한 응답을 유도.\n","\n","4. **결과 파싱**\n","   - Output Parsers를 통해 LLM의 출력을 특정 포맷으로 처리하여 후속 작업을 자동화.\n","\n","\n","**클래스 계층구조**\n","```\n","BasePromptTemplate\n","├─ PipelinePromptTemplate\n","├─ StringPromptTemplate\n","│  ├─ PromptTemplate\n","│  ├─ FewShotPromptTemplate\n","│  └─ FewShotPromptWithTemplates\n","└─ BaseChatPromptTemplate\n","   ├─ AutoGPTPrompt\n","   └─ ChatPromptTemplate\n","      └─ AgentScratchPadChatPromptTemplate\n","\n","BaseMessagePromptTemplate\n","├─ MessagesPlaceholder\n","└─ BaseStringMessagePromptTemplate\n","   ├─ ChatMessagePromptTemplate\n","   ├─ HumanMessagePromptTemplate\n","   ├─ AIMessagePromptTemplate\n","   └─ SystemMessagePromptTemplate\n","\n","```"],"metadata":{"id":"gkQKMY-72cO0"}},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","\n","llm = ChatOpenAI(model_name='gpt-4o-mini')\n","\n","llm.invoke('LLM이 뭔가요?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pfTEDC_e3VKR","executionInfo":{"status":"ok","timestamp":1751002300250,"user_tz":-540,"elapsed":4600,"user":{"displayName":"Jh K","userId":"12877179054230177413"}},"outputId":"6b35822a-c227-4f92-e953-240eb65f24d5"},"execution_count":8,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='LLM은 \"Large Language Model\"의 약자로, 대규모 언어 모델을 의미합니다. 이러한 모델은 인공지능(AI) 기술의 한 분야로, 자연어 처리(NLP) 작업을 수행하기 위해 설계되었습니다. 대량의 텍스트 데이터를 학습하여 언어의 패턴, 의미, 문법 등을 이해하고, 다양한 언어 관련 작업 (예: 번역, 요약, 질문 답변 등)을 수행할 수 있습니다.\\n\\nLLM은 주로 딥러닝 기술을 기반으로 하며, 트랜스포머(Transformer) 아키텍처와 같은 고급 신경망 구조를 사용하여 개발됩니다. 대표적인 예로 OpenAI의 GPT 시리즈, Google\\'s BERT, 그리고 Facebook의 RoBERTa 등이 있습니다. 이러한 모델들은 인간과 비슷한 방식으로 텍스트를 생성하거나 이해할 수 있는 능력을 갖추고 있어, 여러 분야에서 활용되고 있습니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 201, 'prompt_tokens': 15, 'total_tokens': 216, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-BmvySPuAlRm6zJ5qVY5cPnXC4zCoN', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--164e3df8-3f8f-4500-802e-8f1ce10f5b22-0', usage_metadata={'input_tokens': 15, 'output_tokens': 201, 'total_tokens': 216, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","source":["messages = [\n","    ('system', '당신은 친절한 초댕전용 챗봇입니다. 초딩의 눈높이에 맞게 설명해주세요.'),\n","    ('human', '랭체인이 뭔가요?')\n","]\n","\n","llm.invoke(messages)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"u8f3u-5w3qxs","executionInfo":{"status":"ok","timestamp":1751002455637,"user_tz":-540,"elapsed":3490,"user":{"displayName":"Jh K","userId":"12877179054230177413"}},"outputId":"cb1c180b-2107-4bf6-a65f-90bfbd21269b"},"execution_count":9,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='랭체인(Chain of Thought)이라고 해요! 정말 재미있는 것이죠. 사람들이 생각을 정리하거나 문제를 풀 때, 여러 단계를 거쳐서 생각을 이어가는 방식을 뜻해요. \\n\\n예를 들어, 수학 문제를 풀 때 그냥 정답만 생각하는 게 아니라, 문제를 읽고 이해하고, 어떤 방법이 있을지 고민하고, 그 방법을 적용해서 계산한 다음에 마지막으로 정답을 내는 거예요. 이렇게 여러 단계를 차례로 따라가면서 생각하는 걸 랭체인이라고 해요.\\n\\n이런 방식은 특히 컴퓨터나 인공지능이 문제를 더 잘 풀도록 도와줄 수 있답니다! 그래서 랭체인은 요즘 사람들이 인공지능을 만들 때 많이 사용하는 방법 중 하나예요. 이해가 좀 쉽게 되었나요?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 177, 'prompt_tokens': 48, 'total_tokens': 225, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-Bmw0yPDzAbVomMRCerdCupRK38iaZ', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--eb57bf87-ee26-417d-ad7a-dc66a71f6b7f-0', usage_metadata={'input_tokens': 48, 'output_tokens': 177, 'total_tokens': 225, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"]},"metadata":{},"execution_count":9}]},{"cell_type":"markdown","source":["### PromptTemplate"],"metadata":{"id":"Rao_oUQ_4RLp"}},{"cell_type":"code","source":["from langchain import PromptTemplate\n","\n","# 어떤 상품에 대한 광고문구를 생성\n","\n","prompt_template = PromptTemplate(\n","    template='{product}를 홍보하기 위한 신박한 광고문구를 작성해줘.',\n","    input_variables=['product']\n",")\n","prompt = prompt_template.format(product='초소형 카메라')\n","prompt = prompt_template.format(product='냉털전용 냉장고')\n","\n","ai_message = llm.invoke(prompt)\n","print(ai_message.content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9cltJZfx4QrU","executionInfo":{"status":"ok","timestamp":1751002770274,"user_tz":-540,"elapsed":3436,"user":{"displayName":"Jh K","userId":"12877179054230177413"}},"outputId":"dde4a114-6327-440c-e5f9-0eaa2b8b1ea7"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stdout","text":["\"시원한 여름, 나만의 냉장고! 🌬️✨  \n","냉털전용 냉장고로 여름의 더위를 이기고, 신선함을 가득 담아보세요!  \n","차가운 음료, 신선한 과일 – 모든 것이 당신을 기다립니다!  \n","당신의 작은 공간에 큰 시원함을, 냉털의 새로운 세계로 초대합니다!\"  \n","\n","여름에도 걱정 없이 휘어지는 가격, 지금 경험해보세요! 🥶🍹\n"]}]},{"cell_type":"markdown","source":["### ChatPromptTemplate"],"metadata":{"id":"ISEZU12v5xpp"}},{"cell_type":"code","source":["from langchain.prompts.chat import (\n","    ChatPromptTemplate,\n","    SystemMessagePromptTemplate,\n","    HumanMessagePromptTemplate\n",")\n","\n","system_msg_template = SystemMessagePromptTemplate.from_template(\"당신은 {domain}분야의 최고의 챗봇입니다.\")\n","human_msg_template = HumanMessagePromptTemplate.from_template(\"{question}\")\n","chat_template = ChatPromptTemplate.from_messages([\n","    system_msg_template, human_msg_template\n","])\n","\n","prompt = chat_template.format_messages(domain='IT', question='LLM이 뭐야?')\n","\n","llm.invoke(prompt).content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":72},"id":"bL_oGG2I5zL-","executionInfo":{"status":"ok","timestamp":1751003260369,"user_tz":-540,"elapsed":3245,"user":{"displayName":"Jh K","userId":"12877179054230177413"}},"outputId":"c3885bca-62b9-481e-8d93-84460d3bbf4e"},"execution_count":20,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'LLM은 \"Large Language Model\"의 약자로, 대량의 텍스트 데이터를 기반으로 학습한 인공지능 모델을 의미합니다. 이러한 모델은 자연어 처리(NLP) 작업을 수행하는 데 사용되며, 텍스트 생성, 번역, 질문 응답, 요약 등 다양한 작업에 활용됩니다.\\n\\nLLM은 수십억 개의 매개변수를 가지고 있어 복잡한 언어 패턴과 의미를 이해하고 생성할 수 있는 능력을 가지고 있습니다. 대표적인 예로 OpenAI의 GPT(Generative Pre-trained Transformer) 시리즈와 Google의 BERT 모델 등이 있습니다. LLM은 강력하지만, 훈련 데이터의 편향을 반영할 수 있고, 항상 정확한 정보를 제공하지 않을 수 있다는 점에서 주의가 필요합니다.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":20}]},{"cell_type":"code","source":["prompt = chat_template.format_messages(domain='육아', question='우리 아이가 밥을 잘 안먹어요')\n","print(llm.invoke(prompt).content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GRoVF3kt7W8-","executionInfo":{"status":"ok","timestamp":1751003303945,"user_tz":-540,"elapsed":7381,"user":{"displayName":"Jh K","userId":"12877179054230177413"}},"outputId":"f4229228-50b3-4ae3-d351-31bc2d85c66f"},"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["아이의 식사 문제는 많은 부모들이 겪는 고민입니다. 아이가 밥을 잘 안 먹는 이유는 여러 가지가 있을 수 있는데, 몇 가지 해결 방법을 제안드릴게요.\n","\n","1. **식사 환경 조성**: 아이가 편안하게 식사할 수 있는 환경을 만들어 주세요. 소음이 적고, 둘러앉아 식사하는 것이 좋습니다.\n","\n","2. **작은 양으로 시작**: 아이가 한 번에 많이 먹는 것보다 작고 다양한 음식들을 조금씩 제공해 보세요.\n","\n","3. **식사 시간 정하기**: 규칙적인 식사 시간을 정해 아이가 언제 식사할지 알 수 있도록 해 주세요. \n","\n","4. **아이의 취향 존중**: 아이가 좋아하는 음식과 싫어하는 음식을 파악하고 그에 맞춰 메뉴를 구성해 보세요. \n","\n","5. **즐겁게 먹기**: 음식을 재미있게 꾸미거나, 색깔 있는 채소를 사용해 보는 등 식사 시간을 흥미롭게 만들어 주세요.\n","\n","6. **모델링**: 부모가 식사하는 모습을 보여주는 것도 중요합니다. 아이는 부모의 행동을 보고 배우기 때문에, 부모가 건강한 음식을 먹는 모습을 보여주세요.\n","\n","7. **강요는 피하기**: 아이에게 압박을 주지 마세요. 음식은 긍정적인 경험이어야 하므로, 아이가 스스로 먹고 싶도록 유도하는 것이 좋습니다.\n","\n","이 외에도 아이의 성장단계나 개별 상황에 따라 다양한 요인들이 있을 수 있으니, 필요한 경우 소아과 의사와 상담해 보시는 것도 좋은 방법입니다.\n"]}]},{"cell_type":"markdown","source":["## FewshotPromptTemplate"],"metadata":{"id":"MIFo62gyCiXG"}},{"cell_type":"code","source":["from langchain.prompts import FewShotPromptTemplate\n","\n","examples = [\n","    {'q': '2 + 2 = ?', 'a':'4'},\n","    {'q': '3 + 5 = ?', 'a':'8'},\n","]\n","\n","prompt_template = PromptTemplate(\n","    template='Q: {q}\\nA: {a}',\n","    input_variables=['q', 'a']\n",")\n","\n","fewshot_template = FewShotPromptTemplate(\n","    examples=examples,\n","    example_prompt=prompt_template,\n","    prefix='다음 수학 문제를 풀어주세요: ',\n","    suffix='Q: {question} \\nA:', # 사용자 입력값\n","    input_variables=['question']\n",")\n","\n","prompt = fewshot_template.format(question='123+456 = ?')\n","print(prompt)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4L02Cd48CnKP","executionInfo":{"status":"ok","timestamp":1751005811061,"user_tz":-540,"elapsed":26,"user":{"displayName":"Jh K","userId":"12877179054230177413"}},"outputId":"c39b545b-2c86-41bf-8a40-0d1e68b48330"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["다음 수학 문제를 풀어주세요: \n","\n","Q: 2 + 2 = ?\n","A: 4\n","\n","Q: 3 + 5 = ?\n","A: 8\n","\n","Q: 123+456 = ? \n","A:\n"]}]},{"cell_type":"code","source":["print(llm.invoke(prompt).content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-zNAt_xEE61F","executionInfo":{"status":"ok","timestamp":1751005813190,"user_tz":-540,"elapsed":401,"user":{"displayName":"Jh K","userId":"12877179054230177413"}},"outputId":"9d044fe1-4c6b-499e-adbc-ab7b7b5e51ff"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["123 + 456 = 579입니다.\n"]}]},{"cell_type":"markdown","source":["## Output Parsers\n","\n","https://python.langchain.com/api_reference/langchain/output_parsers.html#module-langchain.output_parsers\n","\n","LangChain의 Output Parsers는 LLM이 생성한 텍스트 출력을 특정 형식으로 변환하거나 처리하는 데 사용된다. 이는 모델의 응답을 해석하고, 이를 구조화된 데이터로 바꿔 후속 작업에 활용하기 위해 설계되었다. Output Parsers는 LangChain의 응답 처리 워크플로우에서 중요한 역할을 한다.\n","\n","예를 들어, LLM 응답이 \"Name: John, Age: 30\"와 같은 텍스트라면, 이를 {\"name\": \"John\", \"age\": 30}과 같은 Python 딕셔너리로 변환 가능.\n","\n","**사용 목적**\n","- 모델의 출력을 특정 애플리케이션에 맞게 처리해야 하는 경우가 많음.\n","- 응답을 해석하는 일관성과 정확성을 높이기 위해 필요.\n","- 텍스트 기반 응답을 JSON, 리스트 또는 숫자와 같은 특정 포맷으로 변환하여 후속 작업에 활용.\n","\n","**종류**\n","1. **BaseOutputParser**: Output Parsers의 기본 클래스, 커스텀 파서 구현 시 사용.  \n","2. **CommaSeparatedListOutputParser**: 콤마로 구분된 문자열을 리스트로 변환.  \n","3. **RegexParser**: 정규식을 사용해 특정 패턴을 추출하고 키-값 형태로 반환.  \n","4. **StructuredOutputParser**: 출력의 JSON 또는 구조화된 형식을 강제.  \n","5. **PydanticOutputParser**: Pydantic 모델을 기반으로 출력 검증 및 변환.  \n","6. **MarkdownOutputParser**: 마크다운 형식의 텍스트에서 데이터를 추출.  "],"metadata":{"id":"Fo1NEnF3FGij"}},{"cell_type":"markdown","source":["### CommaSeparatedListOutputParser"],"metadata":{"id":"8Y1nqy7dFI9i"}},{"cell_type":"code","source":["from langchain.output_parsers import CommaSeparatedListOutputParser\n","\n","model_output = \"사과, 바나나, 오렌지, 포도\"\n","\n","output_parser = CommaSeparatedListOutputParser()\n","output = output_parser.parse(model_output)\n","output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ASc5a4OPFF9F","executionInfo":{"status":"ok","timestamp":1751005909635,"user_tz":-540,"elapsed":7,"user":{"displayName":"Jh K","userId":"12877179054230177413"}},"outputId":"04dcabc9-ef76-47cb-af71-9b0258e329de"},"execution_count":36,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['사과', '바나나', '오렌지', '포도']"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["# {야구}팀 {5}개 질문\n","# {축구}팀 {10}개 질문\n","\n","prompt_template = PromptTemplate(\n","    template=\"{subject} {n}개의 팀을 보여주세요.\\n{format_instruction}\",\n","    input_variables=['subject', 'n'],\n","    partial_variables={\n","        'format_instruction': output_parser.get_format_instructions()\n","    }\n",")\n","\n","prompt = prompt_template.format(subject='대한민국 프로야구', n=5)\n","prompt = prompt_template.format(subject=\"프리미어리그\", n=5)\n","prompt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"MA57gDuMFxt8","executionInfo":{"status":"ok","timestamp":1751006470836,"user_tz":-540,"elapsed":7,"user":{"displayName":"Jh K","userId":"12877179054230177413"}},"outputId":"bd6c105e-a4f2-45a6-f0cf-fbea09cf0625"},"execution_count":51,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'프리미어리그 5개의 팀을 보여주세요.\\nYour response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":51}]},{"cell_type":"code","source":["ai_message = llm.invoke(prompt)\n","output = ai_message.content\n","output = output_parser.parse(output)\n","output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OQlpzn-PHmG-","executionInfo":{"status":"ok","timestamp":1751006518515,"user_tz":-540,"elapsed":741,"user":{"displayName":"Jh K","userId":"12877179054230177413"}},"outputId":"f0b09660-ab0f-4856-9549-96b8c59743be"},"execution_count":53,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['맨체스터 시티', '리버풀', '맨체스터 유나이티드', '첼시', '아스널']"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","source":["chain = prompt_template | llm | output_parser   # |: chain 연산자\n","# prompt_template의 출력이 llm에 입력이 되고, llm의 출력이 output_parser에 입력되는 것.\n","chain.invoke(input={'subject':'국내 스트릿댄서', 'n':3})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OdxDAN_AIU54","executionInfo":{"status":"ok","timestamp":1751006942235,"user_tz":-540,"elapsed":486,"user":{"displayName":"Jh K","userId":"12877179054230177413"}},"outputId":"6363107e-bdc0-47ca-d7f9-7b84f921bd78"},"execution_count":76,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['선미', '프라우드먼', '마스터피스']"]},"metadata":{},"execution_count":76}]},{"cell_type":"markdown","source":["### JSONOutputParser"],"metadata":{"id":"u14ylAkFJatt"}},{"cell_type":"code","source":["from langchain_core.output_parsers import JsonOutputParser\n","\n","model_output = '{\"title\": \"GPT-5를 소개합니다.\", \"author\": \"OpenAI\", \"pages\": 250}'\n","\n","json_parser = JsonOutputParser()\n","print(json_parser.get_format_instructions())\n","\n","output = json_parser.parse(model_output) # json_str -> python object(list, dict)\n","output, type(output)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tXXzZB0sJYDp","executionInfo":{"status":"ok","timestamp":1751007904814,"user_tz":-540,"elapsed":13,"user":{"displayName":"Jh K","userId":"12877179054230177413"}},"outputId":"1893d78e-4619-47f0-8923-4d0cd0b755f8"},"execution_count":80,"outputs":[{"output_type":"stream","name":"stdout","text":["Return a JSON object.\n"]},{"output_type":"execute_result","data":{"text/plain":["({'title': 'GPT-5를 소개합니다.', 'author': 'OpenAI', 'pages': 250}, dict)"]},"metadata":{},"execution_count":80}]},{"cell_type":"code","source":["# AI 관련 책 3권을 보여주세요. (json)\n","# 요리 관련 책 5권을 보여주세요.\n","# PromptTemplate - LLM - JsonOutputParser\n","\n","prompt_template = PromptTemplate(\n","    template=\"{subject} 관련 책 {n}권을 보여주세요.\\n{format_instruction}\",\n","    input_variables=['subject', 'n'],\n","    partial_variables={\n","        'format_instruction': json_parser.get_format_instructions()\n","    }\n",")\n","\n","prompt = prompt_template.format(subject='AI', n=3)\n","prompt = prompt_template.format(subject=\"요리\", n=5)\n","prompt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"m39oPYd8NMGs","executionInfo":{"status":"ok","timestamp":1751008484822,"user_tz":-540,"elapsed":20,"user":{"displayName":"Jh K","userId":"12877179054230177413"}},"outputId":"9c7ae0f1-f617-4ec1-89a9-c9e6775cb79c"},"execution_count":96,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'요리 관련 책 5권을 보여주세요.\\nReturn a JSON object.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":96}]},{"cell_type":"code","source":["ai_message = llm.invoke(prompt)\n","output = ai_message.content\n","output = json_parser.parse(output)\n","output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sUluBvAHNnUK","executionInfo":{"status":"ok","timestamp":1751008491679,"user_tz":-540,"elapsed":4983,"user":{"displayName":"Jh K","userId":"12877179054230177413"}},"outputId":"a12c9522-4095-48cf-8b7b-650475326167"},"execution_count":97,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'books': [{'title': 'The Joy of Cooking',\n","   'author': 'Irma S. Rombauer',\n","   'published_year': 1931,\n","   'description': 'A comprehensive cookbook that has been a staple in American kitchens for generations.'},\n","  {'title': 'Salt, Fat, Acid, Heat',\n","   'author': 'Samin Nosrat',\n","   'published_year': 2017,\n","   'description': 'A fundamental guide to mastering the elements of good cooking, with recipes and techniques.'},\n","  {'title': 'Kitchen Confidential',\n","   'author': 'Anthony Bourdain',\n","   'published_year': 2000,\n","   'description': 'An exploration of the world of restaurants and the culinary industry, filled with witty stories and insights.'},\n","  {'title': 'The Food Lab: Better Home Cooking Through Science',\n","   'author': 'J. Kenji López-Alt',\n","   'published_year': 2015,\n","   'description': 'A deep dive into the science of cooking with recipes that aim to demystify everyday cooking.'},\n","  {'title': 'Plenty',\n","   'author': 'Yotam Ottolenghi',\n","   'published_year': 2010,\n","   'description': 'A collection of vibrant vegetarian recipes that celebrate bold flavors and ingredients.'}]}"]},"metadata":{},"execution_count":97}]},{"cell_type":"code","source":["chain = prompt_template | llm | json_parser\n","chain.invoke(input={'subject':'이근삼', 'n':3})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vP4pgEdyNpwr","executionInfo":{"status":"ok","timestamp":1751008577571,"user_tz":-540,"elapsed":2893,"user":{"displayName":"Jh K","userId":"12877179054230177413"}},"outputId":"0859f677-bec1-4032-a3c2-2ad6699f192a"},"execution_count":103,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'books': [{'title': '이근삼의 프로그래밍 언어론',\n","   'author': '이근삼',\n","   'publication_year': 2020},\n","  {'title': '이근삼의 코딩 인사이트', 'author': '이근삼', 'publication_year': 2019},\n","  {'title': '소프트웨어 개발과 이근삼', 'author': '이근삼', 'publication_year': 2021}]}"]},"metadata":{},"execution_count":103}]}]}