{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNJE+YWsOr8tbSC4SKr5ChC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"ac967eb693a346acb2812a866a4eb5d3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_4a20a6f465ee4834a1f2aa20549173f1","IPY_MODEL_f597ce74ecc2486a8084856169bd6da9","IPY_MODEL_5f10f8e1da8240d19c729f4a9d5057e7"],"layout":"IPY_MODEL_4be649ce489548d0b442d70728fad911"}},"4a20a6f465ee4834a1f2aa20549173f1":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c57b12cdeddc4a61a52795c0df2d1990","placeholder":"​","style":"IPY_MODEL_8d5d07c440af47a8badfa4aaf87657e8","value":"Loading checkpoint shards:   0%"}},"f597ce74ecc2486a8084856169bd6da9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"danger","description":"","description_tooltip":null,"layout":"IPY_MODEL_50ac7f2beced4afd84fc02f5bac45cd5","max":2,"min":0,"orientation":"horizontal","style":"IPY_MODEL_58789f20edbd4036bf80bb299c43db49","value":0}},"5f10f8e1da8240d19c729f4a9d5057e7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_100a9a8688a94533b2e808454d4177d6","placeholder":"​","style":"IPY_MODEL_c8aa3b93ef7741f89af59f716cd7c6b6","value":" 0/2 [00:22&lt;?, ?it/s]"}},"4be649ce489548d0b442d70728fad911":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c57b12cdeddc4a61a52795c0df2d1990":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8d5d07c440af47a8badfa4aaf87657e8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"50ac7f2beced4afd84fc02f5bac45cd5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58789f20edbd4036bf80bb299c43db49":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"100a9a8688a94533b2e808454d4177d6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8aa3b93ef7741f89af59f716cd7c6b6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"markdown","source":["# Model IO\n","<img src=\"https://d.pr/i/Wy5B5B+\" width=\"500\"/>\n","\n","- Language Model\n","- Prompt\n","- OutputParser"],"metadata":{"id":"Yd4PfXJ3fvui"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QYMHoI0tfgf4","executionInfo":{"status":"ok","timestamp":1751001584804,"user_tz":-540,"elapsed":15330,"user":{"displayName":"정민영","userId":"01677110233997668601"}},"outputId":"734f0f42-18fb-4ac2-ae06-a9a730f7f3a6"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: langchain in /usr/local/lib/python3.11/dist-packages (0.3.26)\n","Requirement already satisfied: langchain-openai in /usr/local/lib/python3.11/dist-packages (0.3.26)\n","Requirement already satisfied: langchain-community in /usr/local/lib/python3.11/dist-packages (0.3.26)\n","Requirement already satisfied: langchain-huggingface in /usr/local/lib/python3.11/dist-packages (0.3.0)\n","Requirement already satisfied: langchain-core<1.0.0,>=0.3.66 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.66)\n","Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.8 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.3.8)\n","Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.11/dist-packages (from langchain) (0.4.1)\n","Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.11.7)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.0.41)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langchain) (2.32.3)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain) (6.0.2)\n","Requirement already satisfied: openai<2.0.0,>=1.86.0 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (1.91.0)\n","Requirement already satisfied: tiktoken<1,>=0.7 in /usr/local/lib/python3.11/dist-packages (from langchain-openai) (0.9.0)\n","Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (3.11.15)\n","Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (8.5.0)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.6.7)\n","Requirement already satisfied: pydantic-settings<3.0.0,>=2.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.10.1)\n","Requirement already satisfied: httpx-sse<1.0.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (0.4.1)\n","Requirement already satisfied: numpy>=1.26.2 in /usr/local/lib/python3.11/dist-packages (from langchain-community) (2.0.2)\n","Requirement already satisfied: tokenizers>=0.19.1 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.21.1)\n","Requirement already satisfied: huggingface-hub>=0.30.2 in /usr/local/lib/python3.11/dist-packages (from langchain-huggingface) (0.33.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.7.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (6.5.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (0.3.2)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain-community) (1.20.1)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (3.26.1)\n","Requirement already satisfied: typing-inspect<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain-community) (0.9.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (3.18.0)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (2025.3.2)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (24.2)\n","Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (4.67.1)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (4.14.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.30.2->langchain-huggingface) (1.1.5)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<1.0.0,>=0.3.66->langchain) (1.33)\n","Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (3.10.18)\n","Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n","Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith>=0.1.17->langchain) (0.23.0)\n","Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (4.9.0)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.9.0)\n","Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (0.10.0)\n","Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from openai<2.0.0,>=1.86.0->langchain-openai) (1.3.1)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n","Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.1)\n","Requirement already satisfied: python-dotenv>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from pydantic-settings<3.0.0,>=2.4.0->langchain-community) (1.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langchain) (2025.6.15)\n","Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.3)\n","Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.11.6)\n","Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n","Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.66->langchain) (3.0.0)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain-community) (1.1.0)\n"]}],"source":["!pip install langchain langchain-openai langchain-community langchain-huggingface"]},{"cell_type":"code","source":["# colab secret 모두 등록할 것\n","from google.colab import userdata\n","import os\n","\n","\"\"\"\n","LANGSMITH_TRACING=true\n","LANGSMITH_ENDPOINT=\"https://api.smith.langchain.com\"\n","LANGSMITH_API_KEY=\"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\"\n","LANGSMITH_PROJECT=\"skn14_langchain\"\n","OPENAI_API_KEY=\"<your-openai-api-key>\"\n","\"\"\"\n","\n","os.environ['LANGSMITH_TRACING'] = userdata.get('LANGSMITH_TRACING')\n","os.environ['LANGSMITH_ENDPOINT'] = userdata.get('LANGSMITH_ENDPOINT')\n","os.environ['LANGSMITH_API_KEY'] = userdata.get('LANGSMITH_API_KEY')\n","os.environ['LANGSMITH_PROJECT'] = userdata.get('LANGSMITH_PROJECT')\n","os.environ['OPENAI_API_KEY'] = userdata.get('OPENAI_API_KEY')\n","\n"],"metadata":{"id":"lWnwigX0gNCG","executionInfo":{"status":"ok","timestamp":1751001587485,"user_tz":-540,"elapsed":2672,"user":{"displayName":"정민영","userId":"01677110233997668601"}}},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":["## Language Models\n","\n","https://python.langchain.com/api_reference/reference.html#integrations\n","\n","LangChain의 Integrations 섹션에서는 다양한 다운스트림 LLM 모델과의 연동을 지원하다.\n","\n","이 섹션에서는 OpenAI, Hugging Face, GPT-4 등의 다양한 LLM 모델과 LangChain을 연결하는 방법을 다룬다."],"metadata":{"id":"svcvYOxAgaE3"}},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","\n","\n","llm = ChatOpenAI(model_name='gpt-4o', temperature=1)\n","\n","print(llm.invoke('태국의 수도는 어디인가요?').content)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ssg9nt0GxT5S","executionInfo":{"status":"ok","timestamp":1751001598337,"user_tz":-540,"elapsed":10827,"user":{"displayName":"정민영","userId":"01677110233997668601"}},"outputId":"19f1564d-27b1-45e9-80d5-dbe4e19360d3"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["태국의 수도는 방콕입니다.\n"]}]},{"cell_type":"markdown","source":["### huggingface"],"metadata":{"id":"kipaVg2UyOJn"}},{"cell_type":"code","source":["import os\n","from langchain_huggingface import HuggingFaceEndpoint, ChatHuggingFace\n","\n","os.environ[\"HUGGINGFACEHUB_API_TOKEN\"] = userdata.get('HUGGINGFACEHUB_API_TOKEN')\n","\n","llm = HuggingFaceEndpoint(\n","    repo_id='microsoft/Phi-3-mini-4k-instruct',\n","    task='text-generation'\n",")\n","\n","chat_model = ChatHuggingFace(\n","    llm=llm,\n","    verbose=True\n",")\n","\n","response = chat_model.invoke(\"프랑스의 수도는 어디인가요?\")\n","print(response.content)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cLxZcnx7yLie","executionInfo":{"status":"ok","timestamp":1751001609255,"user_tz":-540,"elapsed":10924,"user":{"displayName":"정민영","userId":"01677110233997668601"}},"outputId":"71687f03-1888-4875-c508-120dd8018692"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["프랑스의 수도는 서울으로 위치함에도 불구하고 각 광역시 배면에 있습니다. 서울경 전경인 육지파 수도는 수도광역시인천에서 영천시의 인천고속도로와 연결되어 있습니다. 각 광역시의 수도는 기본적으로 관광학적 발전으로 활용되고 있으며 각 시에 따라 다양한 카페시간과 독특한 전문 이용장을 포함하고 있습니다.\n"]}]},{"cell_type":"code","source":["from langchain_huggingface import HuggingFacePipeline\n","\n","pipe = HuggingFacePipeline.from_model_id(\n","    model_id='microsoft/Phi-3-mini-4k-instruct',\n","    task='text-generation'\n",")\n","pipe.invoke('I love programmi')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423,"referenced_widgets":["ac967eb693a346acb2812a866a4eb5d3","4a20a6f465ee4834a1f2aa20549173f1","f597ce74ecc2486a8084856169bd6da9","5f10f8e1da8240d19c729f4a9d5057e7","4be649ce489548d0b442d70728fad911","c57b12cdeddc4a61a52795c0df2d1990","8d5d07c440af47a8badfa4aaf87657e8","50ac7f2beced4afd84fc02f5bac45cd5","58789f20edbd4036bf80bb299c43db49","100a9a8688a94533b2e808454d4177d6","c8aa3b93ef7741f89af59f716cd7c6b6"]},"id":"g_9xykonz4IM","executionInfo":{"status":"error","timestamp":1751001540516,"user_tz":-540,"elapsed":60899,"user":{"displayName":"정민영","userId":"01677110233997668601"}},"outputId":"4cfcf169-514d-4780-b436-310ed0b4773a"},"execution_count":1,"outputs":[{"output_type":"display_data","data":{"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ac967eb693a346acb2812a866a4eb5d3"}},"metadata":{}},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-1-3925316956.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mlangchain_huggingface\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mHuggingFacePipeline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m pipe = HuggingFacePipeline.from_model_id(\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mmodel_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'microsoft/Phi-3-mini-4k-instruct'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'text-generation'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langchain_huggingface/llms/huggingface_pipeline.py\u001b[0m in \u001b[0;36mfrom_model_id\u001b[0;34m(cls, model_id, task, backend, device, device_map, model_kwargs, pipeline_kwargs, batch_size, **kwargs)\u001b[0m\n\u001b[1;32m    201\u001b[0m             )\n\u001b[1;32m    202\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 203\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_cls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0m_model_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    204\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpad_token\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    572\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4572\u001b[0m                 \u001b[0moffload_index\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4573\u001b[0m                 \u001b[0merror_msgs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4574\u001b[0;31m             \u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_load_pretrained_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4575\u001b[0m                 \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4576\u001b[0m                 \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_pretrained_model\u001b[0;34m(cls, model, state_dict, checkpoint_files, pretrained_model_name_or_path, ignore_mismatched_sizes, sharded_metadata, device_map, disk_offload_folder, offload_state_dict, dtype, hf_quantizer, keep_in_fp32_regex, device_mesh, key_mapping, weights_only)\u001b[0m\n\u001b[1;32m   5029\u001b[0m             \u001b[0;31m# Skip it with fsdp on ranks other than 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5030\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mis_fsdp_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_local_dist_rank_0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_quantized\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5031\u001b[0;31m                 disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(\n\u001b[0m\u001b[1;32m   5032\u001b[0m                     \u001b[0mmodel_to_load\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5033\u001b[0m                     \u001b[0mstate_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_load_state_dict_into_meta_model\u001b[0;34m(model, state_dict, shard_file, expected_keys, reverse_renaming_mapping, device_map, disk_offload_folder, disk_offload_index, cpu_offload_folder, cpu_offload_index, hf_quantizer, is_safetensors, keep_in_fp32_regex, unexpected_keys, device_mesh)\u001b[0m\n\u001b[1;32m    806\u001b[0m             \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcasting_dtype\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 808\u001b[0;31m                 \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcasting_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    809\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mto_contiguous\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m                 \u001b[0mparam\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["from langchain_anthropic import ChatAnthropic\n","from langchain_core.messages import HumanMessage\n","\n","# 모델 초기화\n","model = ChatAnthropic(\n","    model=\"claude-3-opus-20240229\",  # 또는 claude-3-sonnet, claude-3-haiku 등\n","    temperature=0,\n","    max_tokens=1024,\n","    api_key=ANTHROPIC_API_KEY,\n",")\n","\n","# 메시지 구성\n","message = HumanMessage(content=\"프랑스의 수도는 어디인가요?\")\n","\n","# 응답 생성\n","response = model.invoke([message])\n","print(response.content)"],"metadata":{"id":"TI1NDnQD0vZY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ModelLaboratory\n","\n"],"metadata":{"id":"8IPCX0Na1XRv"}},{"cell_type":"code","source":["from langchain.model_laboratory import ModelLaboratory\n","\n","lims = [\n","    ChatOpenAI(model_name='gpt-4', temperature=1),\n","    ChatOpenAI(model_name='gpt-3.5-turbo', temperature=1)\n","]\n","\n","  lab = ModelLaboratory.from_llms(lims)\n","\n","lab.compare('파이썬의 장점이 무엇인가요?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QrjmTYO71t8A","executionInfo":{"status":"ok","timestamp":1751002015020,"user_tz":-540,"elapsed":24432,"user":{"displayName":"정민영","userId":"01677110233997668601"}},"outputId":"1c56fdb4-dd6e-453e-cf33-58bdcd7f084f"},"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[1mInput:\u001b[0m\n","파이썬의 장점이 무엇인가요?\n","\n","client=<openai.resources.chat.completions.completions.Completions object at 0x78c01bc4a050> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x78c01bc49850> root_client=<openai.OpenAI object at 0x78c01bc4bc50> root_async_client=<openai.AsyncOpenAI object at 0x78c01bc49310> model_name='gpt-4' temperature=1.0 model_kwargs={} openai_api_key=SecretStr('**********')\n","\u001b[36;1m\u001b[1;3m1. 간결하고 읽기 쉬운 문법: 파이썬의 간결하고 명확한 문법은 이해하기 쉽고, 코드 작성 시간을 단축시켜줍니다. 또한 이로 인해 유지 보수가 쉽습니다.\n","\n","2. 다양한 분야에 적용: 데이터 분석, 인공지능, 웹 개발, 자동화 스크립팅 등 여러분야에 효율적으로 적용할 수 있습니다.\n","\n","3. 큰 표준 라이브러리 제공: 표준 라이브러리는 필요한 코드를 직접 작성하는 대신, 미리 만들어진 솔루션을 사용할 수 있도록 도와줍니다. \n","\n","4. 활발한 커뮤니티: 파이썬의 크고 활발한 커뮤니티는 많은 패키지들을 제공하며, 이들은 빠르게 진화하는 테크니컬 필드에서도 걸음을 따라잡을 수 있게 해줍니다.\n","\n","5. 동적 타이핑: 파이썬은 런타임에서 타입을 체크하는 동적 타이핑을 지원합니다. 이는 개발 시간을 절약할 수 있습니다.\n","\n","6. 멀티 패러다임: 파이썬은 절차적, 객체지향적, 함수형 프로그래밍을 포함한 여러 패러다임을 지원합니다.\n","\n","7. 크로스 플랫폼: 파이썬은 대부분의 운영체제에서 작동할 수 있으며, 코드를 변경하지 않고도 여러 플랫폼에서 사용할 수 있습니다.\n","\n","8. 인터프리터 언어: 인터프리터 언어로서 일부 코드를 작성하고 즉시 결과를 확인할 수 있어, 빠른 프로토타이핑이 가능합니다.\u001b[0m\n","\n","client=<openai.resources.chat.completions.completions.Completions object at 0x78c01bc86690> async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x78c01bc862d0> root_client=<openai.OpenAI object at 0x78c01bc843d0> root_async_client=<openai.AsyncOpenAI object at 0x78c01bc846d0> temperature=1.0 model_kwargs={} openai_api_key=SecretStr('**********')\n","\u001b[33;1m\u001b[1;3m1. 간결하고 쉬운 문법: 파이썬은 읽기 쉽고 이해하기 쉬운 문법을 사용하여 코딩하는데 있어 학습곡선이 낮습니다.\n","\n","2. 다양한 라이브러리와 프레임워크: 파이썬은 다양한 라이브러리와 프레임워크를 지원하여 빠르고 쉽게 개발할 수 있습니다.\n","\n","3. 크로스 플랫폼 지원: 파이썬은 다양한 플랫폼에서 실행되며 윈도우, 맥, 리눅스 등 다양한 운영체제에서 사용할 수 있습니다.\n","\n","4. 대중적인 언어: 파이썬은 대중적으로 사용되는 프로그래밍 언어로 다양한 분야에서 활용됩니다.\n","\n","5. 커뮤니티와 생태계: 파이썬은 활발한 커뮤니티와 다양한 온라인 자료가 있어서 문제 해결이나 학습에 도움이 됩니다. \n","\n","6. 보안과 안정성: 파이썬은 강력한 보안 및 안정성을 제공하여 안정적인 소프트웨어를 개발할 수 있습니다.\u001b[0m\n","\n"]}]},{"cell_type":"markdown","source":["## Prompts\n","https://python.langchain.com/api_reference/core/prompts.html#langchain-core-prompts\n","\n","`LangChain`의 API 문서에서 제공하는 **Prompts**에 대한 내용은 LangChain 프레임워크의 **핵심 구성 요소 중 하나**로, LLM(Large Language Model)과의 인터페이스를 설정하는 데 중요한 역할을 한다. Prompts는 LLM에 전달될 입력을 정의하고, 구조화하며, 이를 기반으로 원하는 응답을 얻기 위해 사용된다.\n","\n","**주요 사용처**\n","\n","1. **자동화된 입력 구성**\n","   - PromptTemplate을 사용하여 사용자 입력을 자동으로 구성.\n","   - 동일한 형식의 질문이나 대화를 대량으로 생성 가능.\n","\n","2. **대화형 응답**\n","   - ChatPromptTemplate을 통해 대화형 AI의 문맥 유지를 지원.\n","\n","3. **샘플 기반 학습**\n","   - Few-shot Prompt는 LLM에 구체적인 예제를 제공해 정확한 응답을 유도.\n","\n","4. **결과 파싱**\n","   - Output Parsers를 통해 LLM의 출력을 특정 포맷으로 처리하여 후속 작업을 자동화.\n","\n","\n","**클래스 계층구조**\n","```\n","BasePromptTemplate\n","├─ PipelinePromptTemplate\n","├─ StringPromptTemplate\n","│  ├─ PromptTemplate\n","│  ├─ FewShotPromptTemplate\n","│  └─ FewShotPromptWithTemplates\n","└─ BaseChatPromptTemplate\n","   ├─ AutoGPTPrompt\n","   └─ ChatPromptTemplate\n","      └─ AgentScratchPadChatPromptTemplate\n","\n","BaseMessagePromptTemplate\n","├─ MessagesPlaceholder\n","└─ BaseStringMessagePromptTemplate\n","   ├─ ChatMessagePromptTemplate\n","   ├─ HumanMessagePromptTemplate\n","   ├─ AIMessagePromptTemplate\n","   └─ SystemMessagePromptTemplate\n","\n","```\n"],"metadata":{"id":"TX5gntMv2np_"}},{"cell_type":"code","source":["from langchain_openai import ChatOpenAI\n","\n","llm = ChatOpenAI(model_name='gpt-4o-mini')\n","\n","llm.invoke('LLM이 뭔가요?')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oxu37Eoh3ML6","executionInfo":{"status":"ok","timestamp":1751002767336,"user_tz":-540,"elapsed":4249,"user":{"displayName":"정민영","userId":"01677110233997668601"}},"outputId":"b437b4cd-6bb0-4a79-c955-8adc4cd4bb5e"},"execution_count":15,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='LLM은 \"Large Language Model\"의 약자로, 대량의 텍스트 데이터를 기반으로 훈련되어 자연어 처리(NLP) 작업을 수행할 수 있는 인공지능 모델을 의미합니다. 이러한 모델은 문장 생성, 번역, 질문 답변, 요약 등 다양한 작업에 활용될 수 있습니다.\\n\\nLLM은 일반적으로 많은 파라미터를 가지고 있으며, 복잡한 언어 패턴을 학습하여 사람과 유사한 방식으로 언어를 이해하고 생성할 수 있는 능력을 갖추고 있습니다. 예를 들어, OpenAI의 GPT 시리즈나 Google\\'s BERT 등이 LLM의 대표적인 예입니다.\\n\\n이러한 모델들은 제너럴한 지식을 갖추고 있으며, 특정 분야에 대한 전문적인 지식이 필요할 경우 추가적인 학습이나 튜닝이 필요할 수 있습니다.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 182, 'prompt_tokens': 15, 'total_tokens': 197, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-Bmw5zwPtRq43RyMF1kbDJxmd1PXHH', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--2ce5f5ba-026a-47ca-bb44-4bbc9ecf2fb9-0', usage_metadata={'input_tokens': 15, 'output_tokens': 182, 'total_tokens': 197, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"]},"metadata":{},"execution_count":15}]},{"cell_type":"code","source":["messages = [\n","    ('system', '당신은 친절한 초딩전용 챗봇입니다. 초딩의 눈높이에 맞게 설명해주세요.'),\n","    ('human', '랭체인이 뭔가요?')\n","]\n","llm.invoke(messages)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zidQvWQ03nyz","executionInfo":{"status":"ok","timestamp":1751002772058,"user_tz":-540,"elapsed":3435,"user":{"displayName":"정민영","userId":"01677110233997668601"}},"outputId":"d61c369a-ffb7-4894-ec7a-c6e73f9b2417"},"execution_count":16,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='랭체인(RankChain)은 일반적으로 여러 데이터나 정보를 연결하고 정리하는 방법을 이야기해요. 쉽게 말해서, 여러 개의 블록(block)을 연결해서 하나의 줄로 나열하는 것과 비슷해요. 이 블록들은 정보를 담고 있고, 그 정보들은 서로 연결되어 있어요.\\n\\n가령, 만약 당신이 좋아하는 만화책의 내용을 단계별로 정리한다면, 첫 번째 책에서 두 번째 책으로 이어지는 이야기를 연결하는 거예요. 이렇게 하면 이야기를 이해하기 쉬워지죠! 랭체인은 이렇게 데이터를 잘 연결해서 정리하는 데 도움이 되는 도구라고 생각하면 돼요. \\n\\n어떤 질문이 더 궁금한지 물어봐도 좋아요!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 162, 'prompt_tokens': 47, 'total_tokens': 209, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-Bmw64EVdDwKBOWIsfLjHaGv5kDu7w', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--44e5ccac-914f-412e-9a83-d156a93672d6-0', usage_metadata={'input_tokens': 47, 'output_tokens': 162, 'total_tokens': 209, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"]},"metadata":{},"execution_count":16}]},{"cell_type":"markdown","source":["### PromptTemplate"],"metadata":{"id":"4jTNJrO04L6U"}},{"cell_type":"code","source":["from langchain import PromptTemplate\n","\n","# 어떤 상품에 대한 광고문구를 생성\n","\n","prompt_template = PromptTemplate(\n","    template='{product}를 홍보하기 위한 신박한 광고문구를 작성해줘',\n","    input_variables=[\"product\"]\n",")\n","prompt = prompt_template.format(product='초소형 카메라')\n","prompt = prompt_template.format(product='냉털전용 냉장고')\n","\n","llm.invoke(prompt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"datUvduU4Lau","executionInfo":{"status":"ok","timestamp":1751002787603,"user_tz":-540,"elapsed":4383,"user":{"displayName":"정민영","userId":"01677110233997668601"}},"outputId":"fc5aef09-9a5e-4d24-dcf1-9c8d78704396"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["AIMessage(content='\"여름의 열기를 잊게 해줄, 냉털전용 냉장고! 🥶✨  \\n당신의 소중한 간식과 음료를 찜통 더위와 걱정 없이 시원하게!  \\n이제 더 이상 냉장고에 고민은 NO!  \\n차가운 행복, 한 입당 기분 UP!  \\n당신의 여름을 시원하게 만드는 스마트한 선택!  \\n냉털전용 냉장고와 함께 시원한 하루를 만드세요!\"', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 116, 'prompt_tokens': 31, 'total_tokens': 147, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_34a54ae93c', 'id': 'chatcmpl-Bmw6JaTsas5wIjLtlGjGwo1UGgKml', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--bbfdeb7c-3328-4f24-bd26-15e57542c675-0', usage_metadata={'input_tokens': 31, 'output_tokens': 116, 'total_tokens': 147, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"]},"metadata":{},"execution_count":17}]},{"cell_type":"markdown","source":["### ChatPromptTemplate"],"metadata":{"id":"jHk9Z8hg5y-H"}},{"cell_type":"code","source":["from langchain.prompts.chat import (ChatPromptTemplate,\n","                                    SystemMessagePromptTemplate,\n","                                    HumanMessagePromptTemplate)"],"metadata":{"id":"g32-329o503o","executionInfo":{"status":"ok","timestamp":1751002889876,"user_tz":-540,"elapsed":28,"user":{"displayName":"정민영","userId":"01677110233997668601"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["\n","system_msg_template = SystemMessagePromptTemplate.from_template(\"당신은 {domain}분야의 최고의 챗봇입니다.\")\n","human_msg_template = HumanMessagePromptTemplate.from_template(\"{question}\")\n","chat_template = ChatPromptTemplate.from_messages([\n","    system_msg_template, human_msg_template\n","])\n","\n","prompt = chat_template.format_messages(domain='IT', question='LLM이 뭐야?')\n","\n","llm.invoke(prompt).content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":139},"id":"gu_Yo0Tn58rf","executionInfo":{"status":"ok","timestamp":1751003223062,"user_tz":-540,"elapsed":7592,"user":{"displayName":"정민영","userId":"01677110233997668601"}},"outputId":"4f5c1758-8e16-4b3d-de0e-82171d3c1ba0"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'LLM은 \"Large Language Model\"의 약자로, 대규모 언어 모델을 의미합니다. 이러한 모델은 인공지능의 한 분야인 자연어 처리(NLP)에서 사용되며, 방대한 양의 텍스트 데이터를 기반으로 학습하여 인간과 비슷한 방식으로 언어를 이해하고 생성할 수 있는 능력을 갖추고 있습니다.\\n\\nLLM은 다음과 같은 주요 특징들을 가지고 있습니다:\\n\\n1. **규모**: LLM은 수억 또는 수십억 개의 매개변수를 가지고 있어 복잡한 언어 패턴과 규칙을 학습할 수 있습니다.\\n2. **맥락 이해**: 이러한 모델은 문맥을 파악하는 데 뛰어난 능력을 가지고 있으며, 주어진 텍스트의 의미를 이해하고 관련된 응답을 생성할 수 있습니다.\\n3. **다양한 작업**: LLM은 텍스트 생성, 질문 응답, 요약, 번역 등 다양한 자연어 처리 작업을 수행할 수 있습니다.\\n4. **사전 학습과 미세 조정**: 대부분의 LLM은 대규모 데이터셋에서 사전 학습(Pre-training)된 후, 특정 작업에 맞게 미세 조정(Fine-tuning)됩니다.\\n\\n대표적인 LLM으로는 OpenAI의 GPT 시리즈, Google\\'s BERT, T5 등이 있습니다. 이러한 모델들은 언어의 패턴을 이해하고 활용하는 데 매우 유용한 도구로 사용되고 있습니다.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":22}]},{"cell_type":"code","source":["prompt = chat_template.format_messages(domain='육아', question='우리 애가 밥을 안 먹어요')\n","\n","llm.invoke(prompt).content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":156},"id":"CxlKFUfs7QOI","executionInfo":{"status":"ok","timestamp":1751003252931,"user_tz":-540,"elapsed":7943,"user":{"displayName":"정민영","userId":"01677110233997668601"}},"outputId":"b2475a96-7c67-4b07-e8ef-d94f1052e16c"},"execution_count":23,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'아이들이 식사를 거부하는 것은 흔한 일입니다. 여러 가지 이유가 있을 수 있는데, 이런 경우 몇 가지 방법을 시도해 볼 수 있습니다:\\n\\n1. **식사 환경 조성**: 편안하고 조용한 식사 공간을 만들어 주고, 아이가 좋아하는 장식이나 테이블 세팅을 활용해 보세요.\\n\\n2. **식사 시간 규칙 만들기**: 식사 시간을 정해주고, 그 시간에만 밥을 제공하면 아이가 배가 고플 때 자연스럽게 먹으려 할 수 있습니다.\\n\\n3. **다양한 음식**: 다양한 종류의 음식을 시도하여 아이가 좋아하는 맛을 찾는 것이 중요합니다. 색감이 화려한 채소나 고기를 작은 조각으로 잘라내면 아이가 더 관심을 가질 수 있습니다.\\n\\n4. **패턴 변화**: 매일 같은 음식을 주기보다는 새로운 레시피나 조리 방법을 시도해 보세요. 아이가 직접 음식 준비에 참여하게 하면 더 흥미를 느낄 수 있습니다.\\n\\n5. **간식 조절**: 식사 이외의 시간에 너무 많은 간식을 주지 않도록 하고, 식사 때에 배가 고프도록 유도하는 것이 좋습니다.\\n\\n6. **긍정적인 태도**: 아이가 음식을 먹을 때 칭찬하고 격려하는 것이 중요합니다. 압박감을 주기보다는 긍정적인 식습관을 밀어주는 것이 좋습니다.\\n\\n7. **건강 체크**: 만약 아이가 지속적으로 음식을 거부한다면, 건강 문제의 가능성도 고려해 보아야 합니다. pediatrician과 상담해 보는 것이 좋습니다.\\n\\n아이의 식사에 대한 집중과 인내가 필요할 수 있으니, 조금 더 시간을 갖고 기다려 보세요.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":23}]},{"cell_type":"markdown","source":["### FewShotPromptTemplate"],"metadata":{"id":"tk6PZrPDCmnO"}},{"cell_type":"code","source":["from langchain.prompts import FewShotPromptTemplate\n","\n","examples = [\n","    {'q': '2 + 2 = ?', 'a': '4'},\n","    {'q': '3 + 5 = ?', 'a': '8'},\n","]\n","\n","prompt_template = PromptTemplate(\n","    template='Q: {q}\\nA: {a}',\n","    input_variables=['q', 'a']\n",")\n","\n","fewshot_template = FewShotPromptTemplate(\n","    examples = examples,\n","    example_prompt = prompt_template,\n","    prefix = '다음 수학문제를 풀어주세요:',\n","    suffix = 'Q: {question} \\nA:', # 사용자 입력값이 들어와야 할 자리\n","    input_variables = ['question']\n",")\n","\n","prompt = fewshot_template.format(question='1 + 3 * 4')\n","\n","print(prompt)\n","\n","llm.invoke(prompt).content\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":226},"id":"ZaTYtlseCo6q","executionInfo":{"status":"ok","timestamp":1751005568161,"user_tz":-540,"elapsed":4787,"user":{"displayName":"정민영","userId":"01677110233997668601"}},"outputId":"6acabc1a-e2a4-47b9-c847-ad6e95f62854"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["다음 수학문제를 풀어주세요:\n","\n","Q: 2 + 2 = ?\n","A: 4\n","\n","Q: 3 + 5 = ?\n","A: 8\n","\n","Q: 1 + 3 * 4 \n","A:\n"]},{"output_type":"execute_result","data":{"text/plain":["'주어진 수학 문제는 1 + 3 * 4입니다. 여기에서 곱셈이 덧셈보다 우선하므로, 먼저 3 * 4를 계산해야 합니다.\\n\\n3 * 4 = 12\\n\\n그 다음에 1을 더합니다:\\n\\n1 + 12 = 13\\n\\n따라서, 답은 13입니다.'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["## Output Parsers\n","\n","https://python.langchain.com/api_reference/langchain/output_parsers.html#module-langchain.output_parsers\n","\n","LangChain의 Output Parsers는 LLM이 생성한 텍스트 출력을 특정 형식으로 변환하거나 처리하는 데 사용된다. 이는 모델의 응답을 해석하고, 이를 구조화된 데이터로 바꿔 후속 작업에 활용하기 위해 설계되었다. Output Parsers는 LangChain의 응답 처리 워크플로우에서 중요한 역할을 한다.\n","\n","예를 들어, LLM 응답이 \"Name: John, Age: 30\"와 같은 텍스트라면, 이를 {\"name\": \"John\", \"age\": 30}과 같은 Python 딕셔너리로 변환 가능.\n","\n","**사용 목적**\n","- 모델의 출력을 특정 애플리케이션에 맞게 처리해야 하는 경우가 많음.\n","- 응답을 해석하는 일관성과 정확성을 높이기 위해 필요.\n","- 텍스트 기반 응답을 JSON, 리스트 또는 숫자와 같은 특정 포맷으로 변환하여 후속 작업에 활용.\n","\n","**종류**\n","1. **BaseOutputParser**: Output Parsers의 기본 클래스, 커스텀 파서 구현 시 사용.  \n","2. **CommaSeparatedListOutputParser**: 콤마로 구분된 문자열을 리스트로 변환.  \n","3. **RegexParser**: 정규식을 사용해 특정 패턴을 추출하고 키-값 형태로 반환.  \n","4. **StructuredOutputParser**: 출력의 JSON 또는 구조화된 형식을 강제.  \n","5. **PydanticOutputParser**: Pydantic 모델을 기반으로 출력 검증 및 변환.  \n","6. **MarkdownOutputParser**: 마크다운 형식의 텍스트에서 데이터를 추출.  "],"metadata":{"id":"LJcOwml2Ey4y"}},{"cell_type":"markdown","source":["### CommaSeparatedListOutputParser"],"metadata":{"id":"IRNNgvijFGiH"}},{"cell_type":"code","source":["from langchain.output_parsers import CommaSeparatedListOutputParser\n","\n","output_parser = CommaSeparatedListOutputParser()\n","\n","model_output = \"사과, 바나나, 오렌지, 포도\"\n","\n","output = output_parser.parse(model_output) # 리스트로 변환해줌\n","\n","output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1uv_Z3-iEymr","executionInfo":{"status":"ok","timestamp":1751005933105,"user_tz":-540,"elapsed":43,"user":{"displayName":"정민영","userId":"01677110233997668601"}},"outputId":"0a474a3c-0dce-4a1d-90af-7e2416ad4bca"},"execution_count":33,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['사과', '바나나', '오렌지', '포도']"]},"metadata":{},"execution_count":33}]},{"cell_type":"code","source":["# 야구팀 5개\n","# 축구팀 10개 질문\n","\n","prompt_template = PromptTemplate(\n","    template = \"{subject} {n}개의 팀을 보여주세요. \\n{format_instruction}\",\n","    input_variables = ['subject', 'n'], # 사용자 프롬프트로 채워질 변수\n","    partial_variables = {\n","        'format_instruction': output_parser.get_format_instructions()\n","    }\n",")\n","\n","prompt = prompt_template.format(subject='프리미어리그 축구팀', n=5)\n","print(prompt)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"X04_qBJwFtCG","executionInfo":{"status":"ok","timestamp":1751006497569,"user_tz":-540,"elapsed":51,"user":{"displayName":"정민영","userId":"01677110233997668601"}},"outputId":"31e697b9-17de-4986-a458-39aec93a2f87"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["프리미어리그 축구팀 5개의 팀을 보여주세요. \n","Your response should be a list of comma separated values, eg: `foo, bar, baz` or `foo,bar,baz`\n"]}]},{"cell_type":"code","source":["ai_message = llm.invoke(prompt)\n","output = ai_message.content\n","\n","# 출력파서가 가공한 최종출력\n","output = output_parser.parse(output)\n","output"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KNpqFLDzHtJ-","executionInfo":{"status":"ok","timestamp":1751006534275,"user_tz":-540,"elapsed":706,"user":{"displayName":"정민영","userId":"01677110233997668601"}},"outputId":"0f83eb2e-a4f0-46e4-8424-48f8faf8f8ce"},"execution_count":40,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['맨체스터 유나이티드', '리버풀', '첼시', '아스널', '맨체스터 시티']"]},"metadata":{},"execution_count":40}]},{"cell_type":"code","source":["chain = prompt_template | llm | output_parser\n","chain.invoke({'subject': '한국프로야구 팀', 'n': 5})"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nw7uy45BIbAN","executionInfo":{"status":"ok","timestamp":1751006716113,"user_tz":-540,"elapsed":688,"user":{"displayName":"정민영","userId":"01677110233997668601"}},"outputId":"dcfab865-a9ea-483d-8df0-422faf89fc9d"},"execution_count":41,"outputs":[{"output_type":"execute_result","data":{"text/plain":["['두산 베어스', '삼성 라이온즈', 'LG 트윈스', '키움 히어로즈', '한화 이글스']"]},"metadata":{},"execution_count":41}]},{"cell_type":"markdown","source":["###JSONOutputParser"],"metadata":{"id":"cZn9J4_DJPj2"}},{"cell_type":"code","source":["from langchain_core.output_parsers import JsonOutputParser\n","\n","json_parser = JsonOutputParser()\n","\n","print(json_parser.get_format_instructions())\n","\n","model_output = '{\"title\": \"GPT-5를 소개합니다.\", \"author\": \"OpenAI\", \"pages\": 250}'\n","\n","type(json_parser.parse(model_output))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uEfD23udJV0z","executionInfo":{"status":"ok","timestamp":1751007897890,"user_tz":-540,"elapsed":43,"user":{"displayName":"정민영","userId":"01677110233997668601"}},"outputId":"8a3ef5b1-64fa-4833-ac5c-93339948d05e"},"execution_count":46,"outputs":[{"output_type":"stream","name":"stdout","text":["Return a JSON object.\n"]},{"output_type":"execute_result","data":{"text/plain":["dict"]},"metadata":{},"execution_count":46}]},{"cell_type":"code","source":["# AI 관련 책 3권을 보여주세요. (JSON)\n","# 요리 관련 책 5권을 보여주세요, (JSON)\n","# PromptTemplate - LLM - JsonOutputParser\n","\n","prompt_template = PromptTemplate(\n","    template = \"{subject}관련 책 {n}개를 보여주세요. \\n{format_instruction}\",\n","    input_variables = ['subject', 'n'], # 사용자 프롬프트로 채워질 변수\n","    partial_variables = {\n","        'format_instruction': json_parser.get_format_instructions()\n","    }\n",")\n","\n","chain = prompt_template | llm | json_parser\n","print(chain.invoke({'subject': 'AI', 'n': 3}))\n","print(chain.invoke({'subject': '요리', 'n':5}))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Kvqk2FxyJbBV","executionInfo":{"status":"ok","timestamp":1751009445603,"user_tz":-540,"elapsed":9036,"user":{"displayName":"정민영","userId":"01677110233997668601"}},"outputId":"69657823-0eca-4f76-8c6d-dd38cf189d7e"},"execution_count":65,"outputs":[{"output_type":"stream","name":"stdout","text":["{'books': [{'title': 'Artificial Intelligence: A Guide to Intelligent Systems', 'author': 'Michael Negnevitsky', 'publication_year': 2011, 'description': 'This book provides an introduction to the principles and methodologies of artificial intelligence, with a focus on practical applications.'}, {'title': 'Deep Learning', 'author': 'Ian Goodfellow, Yoshua Bengio, and Aaron Courville', 'publication_year': 2016, 'description': 'A comprehensive textbook on deep learning that covers the theory, practical applications, and future trends in the field.'}, {'title': 'Superintelligence: Paths, Dangers, Strategies', 'author': 'Nick Bostrom', 'publication_year': 2014, 'description': 'This book explores the potential future scenarios related to artificial superintelligence, discussing the implications and ethical considerations.'}]}\n","{'cooking_books': [{'title': 'The Joy of Cooking', 'author': 'Irma S. Rombauer', 'year': 1931, 'description': 'A comprehensive cookbook that covers a wide range of recipes and cooking techniques.'}, {'title': 'Salt, Fat, Acid, Heat', 'author': 'Samin Nosrat', 'year': 2017, 'description': 'An exploration of the four fundamental elements of good cooking.'}, {'title': 'The Food Lab: Better Home Cooking Through Science', 'author': 'J. Kenji López-Alt', 'year': 2015, 'description': 'A scientific approach to cooking that explains why certain techniques work.'}, {'title': 'How to Cook Everything', 'author': 'Mark Bittman', 'year': 1998, 'description': 'A modern classic that demystifies cooking with straightforward recipes and techniques.'}, {'title': 'Plenty', 'author': 'Yotam Ottolenghi', 'year': 2010, 'description': 'A collection of vibrant and vegetable-focused recipes that celebrate the flavors of the Mediterranean.'}]}\n"]}]},{"cell_type":"markdown","source":[],"metadata":{"id":"qwzs_RwQFsvQ"}}]}